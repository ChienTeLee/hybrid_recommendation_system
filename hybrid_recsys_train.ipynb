{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "environment": {
      "name": "tf2-gpu.2-1.m50",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "hybrid_recsys_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PamQAseGZ7hh",
        "o544WthqZ7hk",
        "vDc1BCY3Z7h0",
        "mo8oOebkZ7h_"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZmRcplLZ7g0",
        "colab_type": "text"
      },
      "source": [
        "# Build Hybrid Recommendation System\n",
        "\n",
        "The objective of this notebook is to create news_recommend_dataset and build a hybrid recommedation system to recommend news for Austrian news website [Kurier.at](https://kurier.at/). The hybrid recommedation model takes content-based features and latent factors as input, and predict the next news aritcle the user may be interested in. First, we combine preprocess dataset with user and item latent factors to create news_recommend_dataset. Next, we apply wide and deep architecture for our hybrid recommendation model. The deep network takes dense embedding features, and the wide network takes sparse features. Finally, create the model with version on ai-platform to perform batch prediction and online prediction.\n",
        "\n",
        "---\n",
        "The implementation is related to the following paper:\n",
        "- [Wide & Deep Learning for Recommender Systems](https://arxiv.org/abs/1606.07792)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxDNej91Z7g1",
        "colab_type": "text"
      },
      "source": [
        "## 1. import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8hxbyKNZ7g2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujrfafMFZ7g5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set constants\n",
        "PROJECT = \"hybrid-recsys-gcp\"\n",
        "BUCKET = \"hybrid-recsys-gcp-bucket\"\n",
        "REGION = 'us-central1'\n",
        "DATASET = 'news_recommend_dataset'\n",
        "TABLE = \"news_recommend\"\n",
        "MODEL=\"hybrid_recsys_trained_model\"\n",
        "VERSION=\"v1\"\n",
        "\n",
        "os.environ[\"PROJECT\"] = PROJECT\n",
        "os.environ[\"BUCKET\"] = BUCKET\n",
        "os.environ[\"REGION\"] = REGION\n",
        "os.environ[\"DATASET\"] = DATASET\n",
        "os.environ[\"TABLE\"] = TABLE\n",
        "os.environ[\"MODEL\"] = MODEL\n",
        "os.environ[\"VERSION\"] = VERSION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcuPYleVZ7g7",
        "colab_type": "text"
      },
      "source": [
        "## 2. create news_recommend_dataset\n",
        "\n",
        "Combine preprocess dataset with user and item latent factors to create \"news_recommend_train.csv\" and \"news_recommend_test.csv\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0crG2C3Z7g7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store_query_result_to_table(query, project_id, dataset_id, table_id, bucket_id, schema, mode):\n",
        "    \"\"\" Execute query, store result in bigquery table, and save result to bucket in csv file.\n",
        "    \n",
        "    Args:\n",
        "        query (str): The query to be executed in bigquery.\n",
        "        project_id (str): The ID of your project.\n",
        "        dataset_id (str): The name of the dataset.\n",
        "        table_id (str): The name for the table.\n",
        "        bucket_id (str): Bucket to store the csv file of query result.\n",
        "        schema (list:bigquery.SchemaField): Schema of the query result.\n",
        "        mode (str): Train or test mode.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    \n",
        "    table = bigquery.Table(\"{}.{}.{}\".format(project_id, dataset_id, table_id + \"_\" + mode), schema=schema)\n",
        "    table = client.create_table(table)\n",
        "    table_ref = client.dataset(dataset_id).table(table_id + \"_\" + mode)\n",
        "    \n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    job_config.destination = table_ref\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_EMPTY\n",
        "        \n",
        "    query_job = client.query(query, job_config=job_config)\n",
        "    result = query_job.result()\n",
        "    \n",
        "    destination_uri = \"gs://{}/{}/{}_{}.csv\".format(bucket_id, dataset_id, table_id, mode)\n",
        "    extract_job = client.extract_table(table_ref, destination_uri)\n",
        "    extract_job.result()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVDL1e5iZ7g_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify train and test query for hybrid dataset\n",
        "hybrid_train_query = \"\"\"\n",
        "SELECT *\n",
        "FROM {}.preprocess_train\n",
        "\n",
        "INNER JOIN {}.user_latent\n",
        "USING (user_id)\n",
        "\n",
        "INNER JOIN {}.item_latent\n",
        "USING (item_id)\n",
        "\"\"\".format(DATASET, DATASET, DATASET)\n",
        "\n",
        "hybrid_test_query = \"\"\"\n",
        "SELECT *\n",
        "FROM {}.preprocess_test\n",
        "\n",
        "INNER JOIN {}.user_latent\n",
        "USING (user_id)\n",
        "\n",
        "INNER JOIN {}.item_latent\n",
        "USING (item_id)\n",
        "\"\"\".format(DATASET, DATASET, DATASET)\n",
        "\n",
        "# specify schema\n",
        "latent_num = 10\n",
        "\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"user_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"item_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"title\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"author\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"category\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"device_brand\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"article_year\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"article_month\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"rating\", \"FLOAT\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"next_item_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"fold\", \"INTEGER\", mode=\"REQUIRED\")\n",
        "    ] + \\\n",
        "    [bigquery.SchemaField(\"user_latent_{}\".format(i), \"FLOAT\", mode=\"REQUIRED\") for i in range(2 * latent_num)] + \\\n",
        "    [bigquery.SchemaField(\"item_latent_{}\".format(i), \"FLOAT\", mode=\"REQUIRED\") for i in range(2 * latent_num)]\n",
        "\n",
        "# create table\n",
        "store_query_result_to_table(hybrid_train_query, PROJECT, DATASET, TABLE, BUCKET, schema, \"train\")\n",
        "store_query_result_to_table(hybrid_test_query, PROJECT, DATASET, TABLE, BUCKET, schema, \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2mpSBrZ7hC",
        "colab_type": "text"
      },
      "source": [
        "## 3. view hybrid dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhiKY2EmZ7hC",
        "colab_type": "code",
        "colab": {},
        "outputId": "a3493c83-0a04-430f-d130-86c6d6b1d9dd"
      },
      "source": [
        "!gsutil cp gs://{BUCKET}/{DATASET}/{TABLE}_train.csv ./{DATASET}/{TABLE}_train.csv\n",
        "!gsutil cp gs://{BUCKET}/{DATASET}/{TABLE}_test.csv ./{DATASET}/{TABLE}_test.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://hybrid-recsys-gcp-bucket/news_recommend_dataset/news_recommend_train.csv...\n",
            "- [1 files][ 92.8 MiB/ 92.8 MiB]                                                \n",
            "Operation completed over 1 objects/92.8 MiB.                                     \n",
            "Copying gs://hybrid-recsys-gcp-bucket/news_recommend_dataset/news_recommend_test.csv...\n",
            "/ [1 files][ 10.4 MiB/ 10.4 MiB]                                                \n",
            "Operation completed over 1 objects/10.4 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn8tV-w4Z7hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(\"./{}/{}_train.csv\".format(DATASET, TABLE))\n",
        "test_df = pd.read_csv(\"./{}/{}_test.csv\".format(DATASET, TABLE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A16IFT4gZ7hJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "6ac0d933-2e79-41c9-e0a5-f2540b1e8df6"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>category</th>\n",
              "      <th>device_brand</th>\n",
              "      <th>article_year</th>\n",
              "      <th>article_month</th>\n",
              "      <th>rating</th>\n",
              "      <th>next_item_id</th>\n",
              "      <th>...</th>\n",
              "      <th>item_latent_10</th>\n",
              "      <th>item_latent_11</th>\n",
              "      <th>item_latent_12</th>\n",
              "      <th>item_latent_13</th>\n",
              "      <th>item_latent_14</th>\n",
              "      <th>item_latent_15</th>\n",
              "      <th>item_latent_16</th>\n",
              "      <th>item_latent_17</th>\n",
              "      <th>item_latent_18</th>\n",
              "      <th>item_latent_19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1333190552069955484</td>\n",
              "      <td>712711</td>\n",
              "      <td>Salz &amp; Pfeffer: DER RINGSMUTH</td>\n",
              "      <td>Admin I</td>\n",
              "      <td>News</td>\n",
              "      <td>unknown</td>\n",
              "      <td>2011</td>\n",
              "      <td>12</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>299918857</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.101198</td>\n",
              "      <td>-0.049584</td>\n",
              "      <td>-0.026941</td>\n",
              "      <td>0.076896</td>\n",
              "      <td>-0.019934</td>\n",
              "      <td>-0.035466</td>\n",
              "      <td>-0.083057</td>\n",
              "      <td>-0.107345</td>\n",
              "      <td>0.059651</td>\n",
              "      <td>0.066023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4140720900060055522</td>\n",
              "      <td>711784</td>\n",
              "      <td>Gert Korentschnig</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>2011</td>\n",
              "      <td>12</td>\n",
              "      <td>0.348878</td>\n",
              "      <td>299903877</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049612</td>\n",
              "      <td>-0.001534</td>\n",
              "      <td>0.086070</td>\n",
              "      <td>-0.094414</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.057736</td>\n",
              "      <td>0.029307</td>\n",
              "      <td>0.025531</td>\n",
              "      <td>-0.092014</td>\n",
              "      <td>0.015555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4708282785068793097</td>\n",
              "      <td>711895</td>\n",
              "      <td>Impressum KURIER.at</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>2016</td>\n",
              "      <td>2</td>\n",
              "      <td>0.100144</td>\n",
              "      <td>714237</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053754</td>\n",
              "      <td>0.134270</td>\n",
              "      <td>0.078980</td>\n",
              "      <td>-0.054587</td>\n",
              "      <td>0.012783</td>\n",
              "      <td>-0.139207</td>\n",
              "      <td>0.094578</td>\n",
              "      <td>0.063092</td>\n",
              "      <td>-0.106944</td>\n",
              "      <td>0.080386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4708282785068793097</td>\n",
              "      <td>711895</td>\n",
              "      <td>Impressum KURIER.at</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>2016</td>\n",
              "      <td>2</td>\n",
              "      <td>0.100144</td>\n",
              "      <td>714237</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053754</td>\n",
              "      <td>0.134270</td>\n",
              "      <td>0.078980</td>\n",
              "      <td>-0.054587</td>\n",
              "      <td>0.012783</td>\n",
              "      <td>-0.139207</td>\n",
              "      <td>0.094578</td>\n",
              "      <td>0.063092</td>\n",
              "      <td>-0.106944</td>\n",
              "      <td>0.080386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7859670241999925524</td>\n",
              "      <td>711895</td>\n",
              "      <td>Impressum KURIER.at</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>2016</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>769899</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053754</td>\n",
              "      <td>0.134270</td>\n",
              "      <td>0.078980</td>\n",
              "      <td>-0.054587</td>\n",
              "      <td>0.012783</td>\n",
              "      <td>-0.139207</td>\n",
              "      <td>0.094578</td>\n",
              "      <td>0.063092</td>\n",
              "      <td>-0.106944</td>\n",
              "      <td>0.080386</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 51 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               user_id  item_id                          title   author  \\\n",
              "0  1333190552069955484   712711  Salz & Pfeffer: DER RINGSMUTH  Admin I   \n",
              "1  4140720900060055522   711784              Gert Korentschnig  unknown   \n",
              "2  4708282785068793097   711895            Impressum KURIER.at  unknown   \n",
              "3  4708282785068793097   711895            Impressum KURIER.at  unknown   \n",
              "4  7859670241999925524   711895            Impressum KURIER.at  unknown   \n",
              "\n",
              "  category device_brand  article_year  article_month    rating  next_item_id  \\\n",
              "0     News      unknown          2011             12  1.000000     299918857   \n",
              "1  unknown      unknown          2011             12  0.348878     299903877   \n",
              "2  unknown      unknown          2016              2  0.100144        714237   \n",
              "3  unknown      unknown          2016              2  0.100144        714237   \n",
              "4  unknown      unknown          2016              2  1.000000        769899   \n",
              "\n",
              "   ...  item_latent_10  item_latent_11  item_latent_12  item_latent_13  \\\n",
              "0  ...       -0.101198       -0.049584       -0.026941        0.076896   \n",
              "1  ...        0.049612       -0.001534        0.086070       -0.094414   \n",
              "2  ...       -0.053754        0.134270        0.078980       -0.054587   \n",
              "3  ...       -0.053754        0.134270        0.078980       -0.054587   \n",
              "4  ...       -0.053754        0.134270        0.078980       -0.054587   \n",
              "\n",
              "   item_latent_14  item_latent_15  item_latent_16  item_latent_17  \\\n",
              "0       -0.019934       -0.035466       -0.083057       -0.107345   \n",
              "1        0.040390       -0.057736        0.029307        0.025531   \n",
              "2        0.012783       -0.139207        0.094578        0.063092   \n",
              "3        0.012783       -0.139207        0.094578        0.063092   \n",
              "4        0.012783       -0.139207        0.094578        0.063092   \n",
              "\n",
              "   item_latent_18  item_latent_19  \n",
              "0        0.059651        0.066023  \n",
              "1       -0.092014        0.015555  \n",
              "2       -0.106944        0.080386  \n",
              "3       -0.106944        0.080386  \n",
              "4       -0.106944        0.080386  \n",
              "\n",
              "[5 rows x 51 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2ARbuxyZ7hL",
        "colab_type": "code",
        "colab": {},
        "outputId": "3c70bedf-1c4e-45bb-bc3d-b88c0bec5bcf"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>category</th>\n",
              "      <th>device_brand</th>\n",
              "      <th>article_year</th>\n",
              "      <th>article_month</th>\n",
              "      <th>rating</th>\n",
              "      <th>next_item_id</th>\n",
              "      <th>...</th>\n",
              "      <th>item_latent_10</th>\n",
              "      <th>item_latent_11</th>\n",
              "      <th>item_latent_12</th>\n",
              "      <th>item_latent_13</th>\n",
              "      <th>item_latent_14</th>\n",
              "      <th>item_latent_15</th>\n",
              "      <th>item_latent_16</th>\n",
              "      <th>item_latent_17</th>\n",
              "      <th>item_latent_18</th>\n",
              "      <th>item_latent_19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4140720900060055522</td>\n",
              "      <td>711784</td>\n",
              "      <td>Gert Korentschnig</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>2011</td>\n",
              "      <td>12</td>\n",
              "      <td>0.348878</td>\n",
              "      <td>299953030</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049612</td>\n",
              "      <td>-0.001534</td>\n",
              "      <td>0.086070</td>\n",
              "      <td>-0.094414</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.057736</td>\n",
              "      <td>0.029307</td>\n",
              "      <td>0.025531</td>\n",
              "      <td>-0.092014</td>\n",
              "      <td>0.015555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9127664803913758563</td>\n",
              "      <td>711895</td>\n",
              "      <td>Impressum KURIER.at</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>2016</td>\n",
              "      <td>2</td>\n",
              "      <td>0.013988</td>\n",
              "      <td>299935287</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053754</td>\n",
              "      <td>0.134270</td>\n",
              "      <td>0.078980</td>\n",
              "      <td>-0.054587</td>\n",
              "      <td>0.012783</td>\n",
              "      <td>-0.139207</td>\n",
              "      <td>0.094578</td>\n",
              "      <td>0.063092</td>\n",
              "      <td>-0.106944</td>\n",
              "      <td>0.080386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4297993338036722171</td>\n",
              "      <td>714935</td>\n",
              "      <td>Caretta Caretta - von Paulus Hochgatterer</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Stars &amp; Kultur</td>\n",
              "      <td>unknown</td>\n",
              "      <td>2011</td>\n",
              "      <td>12</td>\n",
              "      <td>0.177696</td>\n",
              "      <td>714910</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047850</td>\n",
              "      <td>-0.044623</td>\n",
              "      <td>-0.005471</td>\n",
              "      <td>0.038865</td>\n",
              "      <td>0.041595</td>\n",
              "      <td>-0.030398</td>\n",
              "      <td>0.000696</td>\n",
              "      <td>0.036428</td>\n",
              "      <td>-0.046279</td>\n",
              "      <td>-0.042122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4297993338036722171</td>\n",
              "      <td>752542</td>\n",
              "      <td>Der Schüler Gerber - Von Friedrich Torberg</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Stars &amp; Kultur</td>\n",
              "      <td>unknown</td>\n",
              "      <td>2011</td>\n",
              "      <td>12</td>\n",
              "      <td>0.287090</td>\n",
              "      <td>715441</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016148</td>\n",
              "      <td>-0.004918</td>\n",
              "      <td>-0.019822</td>\n",
              "      <td>0.013644</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>-0.012972</td>\n",
              "      <td>-0.035580</td>\n",
              "      <td>-0.036995</td>\n",
              "      <td>-0.019117</td>\n",
              "      <td>0.000158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4297993338036722171</td>\n",
              "      <td>767949</td>\n",
              "      <td>Der Spion  der aus der Kälte kam - Von John le...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Stars &amp; Kultur</td>\n",
              "      <td>unknown</td>\n",
              "      <td>2012</td>\n",
              "      <td>2</td>\n",
              "      <td>0.089651</td>\n",
              "      <td>767943</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.015711</td>\n",
              "      <td>0.043857</td>\n",
              "      <td>-0.018272</td>\n",
              "      <td>-0.020719</td>\n",
              "      <td>-0.045498</td>\n",
              "      <td>0.018275</td>\n",
              "      <td>0.021870</td>\n",
              "      <td>-0.043413</td>\n",
              "      <td>-0.041688</td>\n",
              "      <td>0.016495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 51 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               user_id  item_id  \\\n",
              "0  4140720900060055522   711784   \n",
              "1  9127664803913758563   711895   \n",
              "2  4297993338036722171   714935   \n",
              "3  4297993338036722171   752542   \n",
              "4  4297993338036722171   767949   \n",
              "\n",
              "                                               title   author        category  \\\n",
              "0                                  Gert Korentschnig  unknown         unknown   \n",
              "1                                Impressum KURIER.at  unknown         unknown   \n",
              "2          Caretta Caretta - von Paulus Hochgatterer  unknown  Stars & Kultur   \n",
              "3         Der Schüler Gerber - Von Friedrich Torberg  unknown  Stars & Kultur   \n",
              "4  Der Spion  der aus der Kälte kam - Von John le...  unknown  Stars & Kultur   \n",
              "\n",
              "  device_brand  article_year  article_month    rating  next_item_id  ...  \\\n",
              "0      unknown          2011             12  0.348878     299953030  ...   \n",
              "1      Samsung          2016              2  0.013988     299935287  ...   \n",
              "2      unknown          2011             12  0.177696        714910  ...   \n",
              "3      unknown          2011             12  0.287090        715441  ...   \n",
              "4      unknown          2012              2  0.089651        767943  ...   \n",
              "\n",
              "   item_latent_10  item_latent_11  item_latent_12  item_latent_13  \\\n",
              "0        0.049612       -0.001534        0.086070       -0.094414   \n",
              "1       -0.053754        0.134270        0.078980       -0.054587   \n",
              "2        0.047850       -0.044623       -0.005471        0.038865   \n",
              "3       -0.016148       -0.004918       -0.019822        0.013644   \n",
              "4       -0.015711        0.043857       -0.018272       -0.020719   \n",
              "\n",
              "   item_latent_14  item_latent_15  item_latent_16  item_latent_17  \\\n",
              "0        0.040390       -0.057736        0.029307        0.025531   \n",
              "1        0.012783       -0.139207        0.094578        0.063092   \n",
              "2        0.041595       -0.030398        0.000696        0.036428   \n",
              "3        0.001424       -0.012972       -0.035580       -0.036995   \n",
              "4       -0.045498        0.018275        0.021870       -0.043413   \n",
              "\n",
              "   item_latent_18  item_latent_19  \n",
              "0       -0.092014        0.015555  \n",
              "1       -0.106944        0.080386  \n",
              "2       -0.046279       -0.042122  \n",
              "3       -0.019117        0.000158  \n",
              "4       -0.041688        0.016495  \n",
              "\n",
              "[5 rows x 51 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNaoe4NgZ7hN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2TJM9_bZ7hQ",
        "colab_type": "text"
      },
      "source": [
        "## 4. create hybrid_recsys package\n",
        "\n",
        "This is the package for the hybrid recommendation model. The trainer/task.py defines the input arguments for training. In trainer/model.py, the \"create_dataset\" function creates tf.dataset for input features. The \"Hybrid_Recsys_Model\" class defines the tf.keras.Model to take feature inputs and predict the next item the viewer may be interested in. And the \"train_and_export_model\" function defines the custom training loop for training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDQRsaTnZ7hQ",
        "colab_type": "text"
      },
      "source": [
        "The Hybrid_Recsys_Model use the following architecture:\n",
        "\n",
        "<img src=\"img/wide_deep_model.png\" width=\"65%\" height=\"65%\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfJjdOKhZ7hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "mkdir -p hybrid_recsys/trainer\n",
        "touch hybrid_recsys/trainer/__init__.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfVSR0qIZ7hT",
        "colab_type": "code",
        "colab": {},
        "outputId": "9c71a49c-b2c5-4a39-cca4-9bfec2f96318"
      },
      "source": [
        "%%writefile hybrid_recsys/setup.py\n",
        "\n",
        "from setuptools import setup, find_packages\n",
        "\n",
        "REQUIRED_PACKAGES = ['tensorflow-hub==0.8.0']\n",
        "\n",
        "setup(name='trainer',\n",
        "    version='0.1',\n",
        "    packages=find_packages(),\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    include_package_data=True,\n",
        "    description='Setup dependencies for trainer package.'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing hybrid_recsys/setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6i7mj59Z7hW",
        "colab_type": "code",
        "colab": {},
        "outputId": "d59f303d-2edc-452d-a231-f7b717205bd4"
      },
      "source": [
        "%%writefile hybrid_recsys/trainer/task.py\n",
        "\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "from trainer import model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--job-dir\",\n",
        "        help=\"job dir to store training outputs and other data\",\n",
        "        required=True\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--train_data_path\",\n",
        "        help=\"path to import train data\",\n",
        "        required=True\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--test_data_path\",\n",
        "        help=\"path to import test data\",\n",
        "        required=True\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        help=\"output dir to export checkpoints or trained model\",\n",
        "        required=True\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--batch_size\",\n",
        "        help=\"batch size for training\",\n",
        "        type=int,\n",
        "        default=2048\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        help=\"number of epochs for training\",\n",
        "        type=int,\n",
        "        default=1\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--latent_num\",\n",
        "        help=\"number of latent factors for gmf and mlp\",\n",
        "        type=int,\n",
        "        default=10\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--item_id_path\",\n",
        "        help=\"path to import item_id_list.txt\",\n",
        "        required=True\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--author_path\",\n",
        "        help=\"path to import author_list.txt\",\n",
        "        required=True\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--category_path\",\n",
        "        help=\"path to import category_list.txt\",\n",
        "        required=True\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--device_brand_path\",\n",
        "        help=\"path to import device_brand_list.txt\",\n",
        "        required=True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--article_year_path\",\n",
        "        help=\"path to import article_year_list.txt\",\n",
        "        required=True\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--article_month_path\",\n",
        "        help=\"path to import article_month_list.txt\",\n",
        "        required=True\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--save_tb_log_to_bucket\",\n",
        "        help=\"set to save tensorboard logs in bucket\",\n",
        "        default=False,\n",
        "        action=\"store_true\"\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"--bucket_tb_log_path\",\n",
        "        help=\"path to store tensorboard in gcp bucket\",\n",
        "        default=\"gs://hybrid-recsys-gcp-bucket/tensorboard_log/ \"\n",
        "    )\n",
        "    \n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    args = args.__dict__\n",
        "\n",
        "    model.train_and_export_model(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing hybrid_recsys/trainer/task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeOPaQtQZ7hY",
        "colab_type": "code",
        "colab": {},
        "outputId": "7e4ca1c0-0629-4b2e-deca-dace596a37c2"
      },
      "source": [
        "%%writefile hybrid_recsys/trainer/model.py\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import shutil\n",
        "import datetime\n",
        "\n",
        "# create dataset function\n",
        "def create_dataset(path, column_name, label_name, defaults, batch_size, shuffle):\n",
        "    \"\"\" Create tf.dataset from csv file.\n",
        "    \n",
        "    Args:\n",
        "        path (str): Path to the csv file.\n",
        "        column_names (list:str): List of string to specify which columns to use in dataset (including label).\n",
        "        label_name (str): Column name for the label.\n",
        "        defaults (list:str): List of string to set default values for columns.\n",
        "        batch_size (str): Batchsize of the dataset.\n",
        "        shuffle (bool): True for shuffling dataset and False otherwise.\n",
        "\n",
        "    Returns:\n",
        "        (tf.dataset): dataset used for training or testing\n",
        "    \"\"\"\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        file_pattern = path,\n",
        "        select_columns=column_name,\n",
        "        label_name=label_name,\n",
        "        column_defaults = defaults,\n",
        "        batch_size=batch_size,\n",
        "        num_epochs=1,\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# model class\n",
        "class Hybrid_Recsys_Model(tf.keras.Model):\n",
        "    \"\"\" The Hybrid_Recsys_Model class. For training, the model takes input features and predict the probability for index of next item id. \n",
        "    For serving, the model takes input features and predict the next item id directly.\n",
        "\n",
        "    Attributes:\n",
        "        user_id_hash (tf.feature_column.categorical_column_with_hash_bucket): hash bucket column for user_id.\n",
        "        item_id_hash (tf.feature_column.categorical_column_with_hash_bucket): hash bucket column for item_id.\n",
        "        author_hash (tf.feature_column.categorical_column_with_hash_bucket): hash bucket column for author.\n",
        "        device_brand_hash (tf.feature_column.categorical_column_with_hash_bucket): hash bucket column for device_brand.\n",
        "        \n",
        "        user_id_embed (tf.feature_column.embedding_column): embedding column for user_id.\n",
        "        item_id_embed (tf.feature_column.embedding_column): embedding column for item_id.\n",
        "        author_embed (tf.feature_column.embedding_column): embedding column for author.\n",
        "        device_brand_embed (tf.feature_column.embedding_column): embedding column for device_brand.\n",
        "        \n",
        "        author_vocab (tf.feature_column.categorical_column_with_vocabulary_list): vocabulary list column for author.\n",
        "        article_year_vocab (tf.feature_column.categorical_column_with_vocabulary_list): vocabulary list column for article_year.\n",
        "        article_month_vocab (tf.feature_column.categorical_column_with_vocabulary_list): vocabulary list column for article_month.\n",
        "        category_vocab (tf.feature_column.categorical_column_with_vocabulary_list): vocabulary list column for category.\n",
        "        device_brand_vocab (tf.feature_column.categorical_column_with_vocabulary_list): vocabulary list column for device_brand.\n",
        "        \n",
        "        author_indicator (tf.feature_column.indicator_column): indicator colunm for author.\n",
        "        cross_date_indicator (tf.feature_column.indicator_column): indicator colunm for crossing article_year and article_month.\n",
        "        category_indicator (tf.feature_column.indicator_column): indicator colunm for category.\n",
        "        device_brand_indicator (tf.feature_column.indicator_column): indicator colunm for device_brand.\n",
        "        \n",
        "        cross_date (tf.feature_column.crossed_column): crossed column for crossing rticle_year and article_month.\n",
        "        \n",
        "        u_latent_numeric (list:tf.feature_column.numeric_column): list of numeric columns for user latent factors.\n",
        "        i_latent_numeric (list:tf.feature_column.numeric_column): list of numeric columns for item latent factors\n",
        "        \n",
        "        feature_columns_d (list:tf.feature_column): list of feature columns for deep features.\n",
        "        feature_columns_w (list:tf.feature_column): list of feature columns for wide features.\n",
        "        \n",
        "        feature_layer_d (tf.keras.layers.DenseFeatures): dense layer for deep feature column.\n",
        "        feature_layer_d (tf.keras.layers.DenseFeatures): dense layer for wide feature column.\n",
        "        \n",
        "        text_embed_layer (hub.KerasLayer): tenseorflow hub layer (NNLM) for text embedding\n",
        "        \n",
        "        dense_1 (tf.keras.layers.Dense): first dense layer for deep network\n",
        "        dense_2 (tf.keras.layers.Dense): second dense layer for deep network\n",
        "        dense_3 (tf.keras.layers.Dense): third dense layer for deep network\n",
        "        dense_4 (tf.keras.layers.Dense): output layer which takes wide and deep networks and predict index for next item.\n",
        "        \n",
        "        item_id_table (tf.lookup.StaticHashTable): table for converting index to ids\n",
        "    \"\"\"\n",
        "    def __init__(self, item_id_path, author_path, category_path, device_brand_path, article_year_path, article_month_path, latent_num):\n",
        "        \"\"\" init method for Hybrid_Recsys_Model class\n",
        "        \n",
        "        Args:\n",
        "            item_id_path (str): Path to txt file containing unique item ids.\n",
        "            author_path (str): Path to txt file containing unique authors.\n",
        "            category_path (str): Path to txt file containing unique categories.\n",
        "            device_brand_path (str): Path to txt file containing unique device_brands.\n",
        "            article_year_path (str): Path to txt file containing unique article_years.\n",
        "            article_month_path (str): Path to txt file containing unique article_months.\n",
        "            latent_num (int): Number of laten factors (gmf and mlp stream each) for representing user ids and item ids.\n",
        "            \n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(Hybrid_Recsys_Model, self).__init__()\n",
        "        # user_id embed\n",
        "        self.user_id_hash = tf.feature_column.categorical_column_with_hash_bucket('user_id', 100)\n",
        "        self.user_id_embed = tf.feature_column.embedding_column(categorical_column=self.user_id_hash, dimension=5)\n",
        "\n",
        "        # item_id embed\n",
        "        self.item_id_hash = tf.feature_column.categorical_column_with_hash_bucket('item_id', 200)\n",
        "        self.item_id_embed = tf.feature_column.embedding_column(categorical_column=self.item_id_hash, dimension=20)\n",
        "\n",
        "        # author embed\n",
        "        self.author_hash = tf.feature_column.categorical_column_with_hash_bucket('author', 20)\n",
        "        self.author_embed = tf.feature_column.embedding_column(categorical_column=self.author_hash, dimension=10)\n",
        "        \n",
        "        # author indicator\n",
        "        self.author_vocab = tf.feature_column.categorical_column_with_vocabulary_list('author', self.get_list(author_path))\n",
        "        self.author_indicator = tf.feature_column.indicator_column(self.author_vocab)\n",
        "\n",
        "        # cross article year, month\n",
        "        self.article_year_vocab = tf.feature_column.categorical_column_with_vocabulary_list('article_year', self.get_list(article_year_path))\n",
        "        self.article_month_vocab = tf.feature_column.categorical_column_with_vocabulary_list('article_month', self.get_list(article_month_path))\n",
        "        \n",
        "        self.cross_date = tf.feature_column.crossed_column([self.article_year_vocab, self.article_month_vocab], \\\n",
        "                                                           self.get_size(article_year_path) * self.get_size(article_month_path))\n",
        "        self.cross_date_indicator = tf.feature_column.indicator_column(self.cross_date)\n",
        "\n",
        "        # category indicator\n",
        "        self.category_vocab = tf.feature_column.categorical_column_with_vocabulary_list('category', self.get_list(category_path))\n",
        "        self.category_indicator = tf.feature_column.indicator_column(self.category_vocab)\n",
        "\n",
        "        # device_brand embed\n",
        "        self.device_brand_hash= tf.feature_column.categorical_column_with_hash_bucket('device_brand', 10)\n",
        "        self.device_brand_embed = tf.feature_column.embedding_column(categorical_column=self.device_brand_hash, dimension=5)\n",
        "\n",
        "        # device_brand indicator\n",
        "        self.device_brand_vocab = tf.feature_column.categorical_column_with_vocabulary_list('device_brand', self.get_list(device_brand_path))\n",
        "        self.device_brand_indicator = tf.feature_column.indicator_column(self.device_brand_vocab)\n",
        "\n",
        "        # user item latent factors\n",
        "        self.u_latent_numeric = [tf.feature_column.numeric_column(key=\"user_latent_\" + str(i)) for i in range(2 * latent_num)]\n",
        "        self.i_latent_numeric =  [tf.feature_column.numeric_column(key=\"item_latent_\" + str(i)) for i in range(2 * latent_num)]\n",
        "\n",
        "        # combine feature columns to feature layer\n",
        "        self.feature_columns_d =  [self.user_id_embed, self.item_id_embed, self.author_embed, self.device_brand_embed] \\\n",
        "                                    + self.u_latent_numeric + self.i_latent_numeric\n",
        "        self.feature_layer_d = tf.keras.layers.DenseFeatures(self.feature_columns_d)\n",
        "        \n",
        "        self.feature_columns_w = [self.author_indicator, self.cross_date_indicator, self.category_indicator, self.device_brand_indicator]\n",
        "        self.feature_layer_w = tf.keras.layers.DenseFeatures(self.feature_columns_w)\n",
        "\n",
        "        # title tf_hub nnlm embedding\n",
        "        self.text_embed_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-de-dim50-with-normalization/2\", dtype=tf.string)\n",
        "       \n",
        "        # dense\n",
        "        self.dense_1 = tf.keras.layers.Dense(200, activation='relu')\n",
        "        self.dense_2 = tf.keras.layers.Dense(100, activation='relu')\n",
        "        self.dense_3 = tf.keras.layers.Dense(50, activation='relu')\n",
        "        self.dense_4 = tf.keras.layers.Dense(self.get_size(item_id_path) + 1, activation='softmax')\n",
        "\n",
        "        # item_id lookup table\n",
        "        self.item_id_table = self.create_item_id_table(item_id_path)\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        \"\"\"The call method for NeuMF class.\n",
        "\n",
        "        Args:\n",
        "            inputs (OrderedDict:tf.Tensor): OrderedDict of input feature tensor.\n",
        "        \n",
        "        Returns:\n",
        "            output (tf.Tensor): The predicted probability for index of next item id.\n",
        "        \"\"\"\n",
        "        # wide, deep feature columns\n",
        "        feature_cols_d = self.feature_layer_d(inputs)\n",
        "        feature_cols_w = self.feature_layer_w(inputs)\n",
        "        \n",
        "        # title embedding\n",
        "        title = inputs['title']\n",
        "        title_embed = self.text_embed_layer(title)\n",
        "        \n",
        "        # deep network\n",
        "        concat_1 = tf.concat([feature_cols_d, title_embed], axis=1)\n",
        "        dense_1_out = self.dense_1(concat_1)\n",
        "        dense_2_out = self.dense_2(dense_1_out)\n",
        "        dense_3_out = self.dense_3(dense_2_out)\n",
        "        \n",
        "        # combine wide, and deep layers\n",
        "        concat_2 = tf.concat([dense_3_out, feature_cols_w], axis=1)\n",
        "\n",
        "        output = self.dense_4(concat_2)\n",
        "        return output\n",
        "    \n",
        "    \n",
        "    latent_num = 10\n",
        "    key = [\"user_id\", \"item_id\", \"title\", \"author\", \"category\", \"device_brand\", \"article_year\", \"article_month\"] + \\\n",
        "            [\"user_latent_{}\".format(i) for i in range(2*latent_num)] + [\"item_latent_{}\".format(i) for i in range(2*latent_num)]\n",
        "    value = [tf.TensorSpec([None], dtype=tf.string, name=\"user_id\"),\n",
        "                tf.TensorSpec([None], dtype=tf.string, name=\"item_id\"), \\\n",
        "                tf.TensorSpec([None], dtype=tf.string, name=\"title\"), \\\n",
        "                tf.TensorSpec([None], dtype=tf.string, name=\"author\"), \\\n",
        "                tf.TensorSpec([None], dtype=tf.string, name=\"category\"), \\\n",
        "                tf.TensorSpec([None], dtype=tf.string, name=\"device_brand\"), \\\n",
        "                tf.TensorSpec([None], dtype=tf.string, name=\"article_year\"), \\\n",
        "                tf.TensorSpec([None], dtype=tf.string, name=\"article_month\")] + \\\n",
        "            [tf.TensorSpec([None], dtype=tf.float32, name=\"user_latent_{}\".format(i)) for i in range(2*latent_num)] + \\\n",
        "            [tf.TensorSpec([None], dtype=tf.float32, name=\"item_latent_{}\".format(i)) for i in range(2*latent_num)]\n",
        "    signature_dict = dict(zip(key, value))\n",
        "    @tf.function(input_signature=[signature_dict])\n",
        "    def my_serve(self, x):\n",
        "        \"\"\"The serving method for Hybrid class.\n",
        "\n",
        "        Args:\n",
        "            inputs (OrderedDict:tf.Tensor): OrderedDict of input feature tensor.\n",
        "        \n",
        "        Returns:\n",
        "            output (tf.Tensor): The predicted next item id.\n",
        "        \"\"\"\n",
        "        pred = self.__call__(x)\n",
        "        values, indices = tf.math.top_k(pred, k=10)\n",
        "        item_ids = self.item_id_table.lookup(indices)\n",
        "        return {\"top_k_item_id\": item_ids}\n",
        "    \n",
        "    def get_size(self, file_path):\n",
        "        \"\"\"Returns total number of lines in the txt file.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to txt file.\n",
        "        \n",
        "        Returns:\n",
        "            size (int): total number of lines in the txt file.\n",
        "        \"\"\"\n",
        "        id_tensors = tf.strings.split(tf.io.read_file(file_path), '\\n')\n",
        "        return id_tensors.shape[0]\n",
        "    \n",
        "    def get_list(self, file_path):\n",
        "        \"\"\"Extract content from txt file and store into list. Each line in txt file corresponds to an elemnt in list.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to txt file.\n",
        "        \n",
        "        Returns:\n",
        "            id_list (int): list of values from txt file.\n",
        "        \"\"\"\n",
        "        id_tensors = tf.strings.split(tf.io.read_file(file_path), '\\n')\n",
        "        id_list = [tf.compat.as_str_any(x) for x in id_tensors.numpy()]\n",
        "        return id_list\n",
        "    \n",
        "    def create_item_id_table(self, file_path):\n",
        "        \"\"\" create lookup table to translate item index to item id.\n",
        "        \n",
        "        Args:\n",
        "            file_path (str): Path to txt file containing item ids.\n",
        "            \n",
        "        Returns:\n",
        "            (tf.lookup.StaticVocabularyTable): The lookup table.\n",
        "        \"\"\"\n",
        "        values = tf.strings.split(tf.io.read_file(file_path), '\\n')\n",
        "        keys = tf.range(values.shape[0], dtype=tf.int32)\n",
        "        initializer = tf.lookup.KeyValueTensorInitializer(keys=keys, values=values, key_dtype=tf.int32, value_dtype=tf.string)\n",
        "        table = tf.lookup.StaticHashTable(initializer, default_value='unknown')\n",
        "        return table\n",
        "        \n",
        "        \n",
        "    \n",
        "def train_and_export_model(args):\n",
        "    \"\"\" Train the Hybrid_Recsys_Model and export model to bucket.\n",
        "\n",
        "    Args:\n",
        "        args (dict): dict of arguments from task.py\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # create dataset\n",
        "    feature_col = ['user_id', 'item_id', 'title', 'author', 'category', 'device_brand', 'article_year', 'article_month', 'next_item_id']\n",
        "    u_latent_col = [\"user_latent_{}\".format(i) for i in range(2 * args[\"latent_num\"])]\n",
        "    i_latent_col = [\"item_latent_{}\".format(i) for i in range(2 * args[\"latent_num\"])]\n",
        "    column_name = feature_col + u_latent_col + i_latent_col\n",
        "    \n",
        "    label_name = 'next_item_id'\n",
        "    defaults = ['unknown'] * len(feature_col) + [0.0] * (len(u_latent_col) + len(i_latent_col))\n",
        "    batch_size = args[\"batch_size\"]\n",
        "    train_path = args[\"train_data_path\"]\n",
        "    test_path = args[\"test_data_path\"]\n",
        "    \n",
        "    train_dataset = create_dataset(train_path, column_name, label_name, defaults, batch_size, True)\n",
        "    test_dataset = create_dataset(test_path, column_name, label_name, defaults, batch_size, False)\n",
        "    \n",
        "    # create model\n",
        "    model = Hybrid_Recsys_Model(args[\"item_id_path\"], args[\"author_path\"], args[\"category_path\"], args[\"device_brand_path\"], \\\n",
        "                                args[\"article_year_path\"], args[\"article_month_path\"], args[\"latent_num\"])\n",
        "    \n",
        "    # loopup table (convvert id to index)\n",
        "    item_index_initializer = tf.lookup.TextFileInitializer(args[\"item_id_path\"], key_dtype=tf.string, key_index=tf.lookup.TextFileIndex.WHOLE_LINE, \\\n",
        "                            value_dtype=tf.int64, value_index=tf.lookup.TextFileIndex.LINE_NUMBER, delimiter=\"\\n\")\n",
        "    item_index_table = tf.lookup.StaticVocabularyTable(item_index_initializer, num_oov_buckets=1)\n",
        "    \n",
        "    \n",
        "    # loss function and optimizers\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "    \n",
        "    # loss metrics\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "    train_top_10_accuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name='train_top_10_accuracy')\n",
        "\n",
        "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "    test_top_10_accuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name='test_top_10_accuracy')\n",
        "    \n",
        "    # tensorboard\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    train_log_dir = './logs/gradient_tape/' + current_time + '/train'\n",
        "    test_log_dir = './logs/gradient_tape/' + current_time + '/test'\n",
        "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
        "    \n",
        "    @tf.function\n",
        "    def train_step(features, labels):\n",
        "        \"\"\" Concrete function for train setp and update train metircs\n",
        "\n",
        "        Args:\n",
        "            features (OrderedDict:tf.Tensor): OrderedDict of tensor containing input  features.\n",
        "            labels (tf.Tensor): labels indicating the next_item id\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(features, training=True)\n",
        "            label_indicies = item_index_table.lookup(labels)\n",
        "            loss = loss_object(label_indicies, predictions)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(label_indicies, predictions)\n",
        "        train_top_10_accuracy(label_indicies, predictions)\n",
        "    \n",
        "    @tf.function\n",
        "    def test_step(features, labels):\n",
        "        \"\"\" Concrete function for test setp and update test metircs\n",
        "\n",
        "        Args:\n",
        "            features (OrderedDict:tf.Tensor): OrderedDict of tensor containing input features.\n",
        "            labels (tf.Tensor): labels indicating the next_item id\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        predictions = model(features, training=False)\n",
        "        label_indicies = item_index_table.lookup(labels)\n",
        "        loss = loss_object(label_indicies, predictions)\n",
        "\n",
        "        test_loss(loss)\n",
        "        test_accuracy(label_indicies, predictions)\n",
        "        test_top_10_accuracy(label_indicies, predictions)\n",
        "    \n",
        "    # custom train loop\n",
        "    EPOCHS = args[\"epochs\"]\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss.reset_states()\n",
        "        train_accuracy.reset_states()\n",
        "        train_top_10_accuracy.reset_states()\n",
        "\n",
        "        test_loss.reset_states()\n",
        "        test_accuracy.reset_states()\n",
        "        test_top_10_accuracy.reset_states()\n",
        "\n",
        "        for features, labels in train_dataset:\n",
        "            train_step(features, labels)\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "            tf.summary.scalar('top_10_accuracy', train_top_10_accuracy.result(), step=epoch)\n",
        "\n",
        "        for features, labels in test_dataset:\n",
        "            test_step(features, labels)\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "            tf.summary.scalar('top_10_accuracy', test_top_10_accuracy.result(), step=epoch)\n",
        "\n",
        "        template = 'Epoch {:d}, train[loss: {:.6f}, acc: {:.6f}, top_10_acc: {:.6f}], Test[loss: {:.6f}, acc: {:.6f}, top_10_acc: {:.6f}]'\n",
        "\n",
        "        print(template.format(epoch + 1,\n",
        "                              train_loss.result(),\n",
        "                              train_accuracy.result() * 100,\n",
        "                              train_top_10_accuracy.result() * 100,\n",
        "\n",
        "                              test_loss.result(),\n",
        "                              test_accuracy.result() * 100,\n",
        "                              test_top_10_accuracy.result() * 100,\n",
        "                              ))\n",
        "    \n",
        "    # exprot tensorboard log\n",
        "    if args[\"save_tb_log_to_bucket\"]:\n",
        "        script = \"gsutil cp -r ./logs {}\".format(args[\"bucket_tb_log_path\"])\n",
        "        os.system(script)\n",
        "    \n",
        "    # export model\n",
        "    EXPORT_PATH = os.path.join(args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "    tf.saved_model.save(obj=model, export_dir=EXPORT_PATH, signatures={'serving_default': model.my_serve})\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing hybrid_recsys/trainer/model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYb4JwiiZ7hb",
        "colab_type": "text"
      },
      "source": [
        "## 5. train model locally\n",
        "\n",
        "Run package as a python module in local environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Envs_agEZ7hc",
        "colab_type": "code",
        "colab": {},
        "outputId": "18d6f759-06fc-443a-bcd6-0f025c8aa974"
      },
      "source": [
        "%%bash\n",
        "\n",
        "JOBDIR=./${MODEL}\n",
        "OUTDIR=./${MODEL}\n",
        "\n",
        "rm -rf ${JOBDIR}\n",
        "export PYTHONPATH=${PYTHONPATH}:${PWD}/hybrid_recsys\n",
        "\n",
        "python -m trainer.task \\\n",
        "    --job-dir=${JOBDIR} \\\n",
        "    --train_data_path=gs://${BUCKET}/${DATASET}/${TABLE}_train.csv \\\n",
        "    --test_data_path=gs://${BUCKET}/${DATASET}/${TABLE}_test.csv \\\n",
        "    --output_dir=${OUTDIR} \\\n",
        "    --batch_size=2048 \\\n",
        "    --epochs=40 \\\n",
        "    --latent_num=10 \\\n",
        "    --item_id_path=gs://${BUCKET}/${DATASET}/item_id_list.txt \\\n",
        "    --author_path=gs://${BUCKET}/${DATASET}/author_list.txt \\\n",
        "    --category_path=gs://${BUCKET}/${DATASET}/category_list.txt \\\n",
        "    --device_brand_path=gs://${BUCKET}/${DATASET}/device_brand_list.txt \\\n",
        "    --article_year_path=gs://${BUCKET}/${DATASET}/article_year_list.txt \\\n",
        "    --article_month_path=gs://${BUCKET}/${DATASET}/article_month_list.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train[loss: 6.038098, acc: 1.906627, top_10_acc: 17.033777], Test[loss: 5.349120, acc: 2.306068, top_10_acc: 19.401793]\n",
            "Epoch 2, train[loss: 5.288680, acc: 2.978423, top_10_acc: 20.357052], Test[loss: 5.276124, acc: 2.916833, top_10_acc: 22.016096]\n",
            "Epoch 3, train[loss: 5.221677, acc: 3.328410, top_10_acc: 22.089649], Test[loss: 5.207056, acc: 3.299275, top_10_acc: 22.935099]\n",
            "Epoch 4, train[loss: 5.141301, acc: 3.820961, top_10_acc: 23.950039], Test[loss: 5.102207, acc: 4.315315, top_10_acc: 25.229750]\n",
            "Epoch 5, train[loss: 5.006400, acc: 4.811200, top_10_acc: 27.856409], Test[loss: 5.009177, acc: 5.182944, top_10_acc: 28.551859]\n",
            "Epoch 6, train[loss: 4.870391, acc: 5.633830, top_10_acc: 31.545082], Test[loss: 4.858266, acc: 5.519722, top_10_acc: 32.553230]\n",
            "Epoch 7, train[loss: 4.736486, acc: 6.245826, top_10_acc: 34.408554], Test[loss: 4.763878, acc: 6.467264, top_10_acc: 34.648098]\n",
            "Epoch 8, train[loss: 4.646415, acc: 6.771770, top_10_acc: 36.219494], Test[loss: 4.702007, acc: 6.769793, top_10_acc: 36.143616]\n",
            "Epoch 9, train[loss: 4.590014, acc: 7.095428, top_10_acc: 37.562290], Test[loss: 4.642763, acc: 7.277813, top_10_acc: 36.777214]\n",
            "Epoch 10, train[loss: 4.534849, acc: 7.523761, top_10_acc: 38.592987], Test[loss: 4.603028, acc: 7.414807, top_10_acc: 37.827499]\n",
            "Epoch 11, train[loss: 4.490069, acc: 7.825585, top_10_acc: 39.250576], Test[loss: 4.577342, acc: 7.460471, top_10_acc: 38.438267]\n",
            "Epoch 12, train[loss: 4.451998, acc: 8.040714, top_10_acc: 40.060364], Test[loss: 4.551493, acc: 7.757292, top_10_acc: 39.168903]\n",
            "Epoch 13, train[loss: 4.421233, acc: 8.325841, top_10_acc: 40.627407], Test[loss: 4.539711, acc: 7.745876, top_10_acc: 39.123238]\n",
            "Epoch 14, train[loss: 4.395466, acc: 8.585923, top_10_acc: 40.976753], Test[loss: 4.521069, acc: 7.825789, top_10_acc: 39.619843]\n",
            "Epoch 15, train[loss: 4.376716, acc: 8.750321, top_10_acc: 41.625996], Test[loss: 4.507343, acc: 8.310977, top_10_acc: 39.939491]\n",
            "Epoch 16, train[loss: 4.347047, acc: 8.887747, top_10_acc: 42.076805], Test[loss: 4.497812, acc: 7.939951, top_10_acc: 40.481766]\n",
            "Epoch 17, train[loss: 4.331199, acc: 9.160030, top_10_acc: 42.302849], Test[loss: 4.484862, acc: 8.242479, top_10_acc: 40.790001]\n",
            "Epoch 18, train[loss: 4.307646, acc: 9.251862, top_10_acc: 42.736961], Test[loss: 4.482033, acc: 8.345225, top_10_acc: 40.710087]\n",
            "Epoch 19, train[loss: 4.286770, acc: 9.373877, top_10_acc: 42.955303], Test[loss: 4.479535, acc: 8.499344, top_10_acc: 41.195274]\n",
            "Epoch 20, train[loss: 4.274946, acc: 9.564603, top_10_acc: 43.255199], Test[loss: 4.468458, acc: 8.516468, top_10_acc: 41.252354]\n",
            "Epoch 21, train[loss: 4.259432, acc: 9.688543, top_10_acc: 43.600052], Test[loss: 4.467499, acc: 8.761915, top_10_acc: 41.537758]\n",
            "Epoch 22, train[loss: 4.242886, acc: 9.785512, top_10_acc: 43.871693], Test[loss: 4.464984, acc: 8.893202, top_10_acc: 41.440723]\n",
            "Epoch 23, train[loss: 4.229613, acc: 10.042383, top_10_acc: 44.012974], Test[loss: 4.463449, acc: 8.784748, top_10_acc: 41.651920]\n",
            "Epoch 24, train[loss: 4.220156, acc: 10.085410, top_10_acc: 44.344337], Test[loss: 4.463076, acc: 8.824704, top_10_acc: 41.754665]\n",
            "Epoch 25, train[loss: 4.208478, acc: 10.135499, top_10_acc: 44.589008], Test[loss: 4.460273, acc: 9.030196, top_10_acc: 41.863121]\n",
            "Epoch 26, train[loss: 4.191808, acc: 10.240175, top_10_acc: 44.697536], Test[loss: 4.462725, acc: 8.916034, top_10_acc: 41.908787]\n",
            "Epoch 27, train[loss: 4.181005, acc: 10.315310, top_10_acc: 44.935139], Test[loss: 4.463884, acc: 8.944574, top_10_acc: 41.994404]\n",
            "Epoch 28, train[loss: 4.168359, acc: 10.412920, top_10_acc: 45.129078], Test[loss: 4.456953, acc: 9.024487, top_10_acc: 42.154232]\n",
            "Epoch 29, train[loss: 4.158968, acc: 10.601721, top_10_acc: 45.414207], Test[loss: 4.454102, acc: 9.041612, top_10_acc: 42.154232]\n",
            "Epoch 30, train[loss: 4.147163, acc: 10.641536, top_10_acc: 45.531723], Test[loss: 4.455685, acc: 9.355556, top_10_acc: 42.165649]\n",
            "Epoch 31, train[loss: 4.138405, acc: 10.796301, top_10_acc: 45.889416], Test[loss: 4.454064, acc: 9.269936, top_10_acc: 42.102859]\n",
            "Epoch 32, train[loss: 4.126423, acc: 10.874647, top_10_acc: 46.039684], Test[loss: 4.454871, acc: 9.321308, top_10_acc: 42.331184]\n",
            "Epoch 33, train[loss: 4.117373, acc: 10.994734, top_10_acc: 46.201515], Test[loss: 4.456493, acc: 9.355556, top_10_acc: 42.102859]\n",
            "Epoch 34, train[loss: 4.105209, acc: 11.069869, top_10_acc: 46.407013], Test[loss: 4.457365, acc: 9.241395, top_10_acc: 42.154232]\n",
            "Epoch 35, train[loss: 4.097700, acc: 11.292705, top_10_acc: 46.563061], Test[loss: 4.461668, acc: 9.327016, top_10_acc: 42.159943]\n",
            "Epoch 36, train[loss: 4.087977, acc: 11.349859, top_10_acc: 46.692139], Test[loss: 4.462842, acc: 9.298476, top_10_acc: 42.308350]\n",
            "Epoch 37, train[loss: 4.079717, acc: 11.384537, top_10_acc: 46.814796], Test[loss: 4.465126, acc: 9.435471, top_10_acc: 42.256977]\n",
            "Epoch 38, train[loss: 4.070292, acc: 11.635629, top_10_acc: 46.955433], Test[loss: 4.469710, acc: 9.338432, top_10_acc: 42.268394]\n",
            "Epoch 39, train[loss: 4.070684, acc: 11.688929, top_10_acc: 47.047909], Test[loss: 4.469059, acc: 9.338432, top_10_acc: 42.296936]\n",
            "Epoch 40, train[loss: 4.055965, acc: 11.717827, top_10_acc: 47.230927], Test[loss: 4.474806, acc: 9.543924, top_10_acc: 42.142818]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-16 17:40:10.260042: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-08-16 17:40:10.261124: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557d65cfe320 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-16 17:40:10.261158: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-16 17:40:10.261284: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "2020-08-16 17:40:18.288004: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 19 in the outer inference context.\n",
            "2020-08-16 17:40:18.566826: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 19 in the outer inference context.\n",
            "2020-08-16 17:41:01.389106: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 19 in the outer inference context.\n",
            "2020-08-16 18:07:45.728404: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kd57nB1Z7he",
        "colab_type": "text"
      },
      "source": [
        "## 6. train model on cloud\n",
        "\n",
        "Submit a training job in gcloud ai-platform to train the package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cf5GNYrZ7he",
        "colab_type": "code",
        "colab": {},
        "outputId": "580ef610-b3d6-48c6-fe7f-a024bcc12858"
      },
      "source": [
        "%%bash\n",
        "\n",
        "JOBDIR=gs://${BUCKET}/${MODEL}\n",
        "OUTDIR=gs://${BUCKET}/${MODEL}\n",
        "JOBID=hybrid_recsys_train_job_$(date -u +%y%m%d_%H%M%S)\n",
        "\n",
        "gcloud ai-platform jobs submit training ${JOBID} \\\n",
        "    --region=${REGION} \\\n",
        "    --module-name=trainer.task \\\n",
        "    --package-path=$(pwd)/hybrid_recsys/trainer \\\n",
        "    --staging-bucket=gs://${BUCKET} \\\n",
        "    --scale-tier=CUSTOM \\\n",
        "    --master-machine-type=n1-highcpu-16 \\\n",
        "    --runtime-version=2.1 \\\n",
        "    --python-version=3.7 \\\n",
        "    -- \\\n",
        "    --job-dir=${JOBDIR} \\\n",
        "    --train_data_path=gs://${BUCKET}/${DATASET}/${TABLE}_train.csv \\\n",
        "    --test_data_path=gs://${BUCKET}/${DATASET}/${TABLE}_test.csv \\\n",
        "    --output_dir=${OUTDIR} \\\n",
        "    --batch_size=2048 \\\n",
        "    --epochs=40 \\\n",
        "    --latent_num=10 \\\n",
        "    --item_id_path=gs://${BUCKET}/${DATASET}/item_id_list.txt \\\n",
        "    --author_path=gs://${BUCKET}/${DATASET}/author_list.txt \\\n",
        "    --category_path=gs://${BUCKET}/${DATASET}/category_list.txt \\\n",
        "    --device_brand_path=gs://${BUCKET}/${DATASET}/device_brand_list.txt \\\n",
        "    --article_year_path=gs://${BUCKET}/${DATASET}/article_year_list.txt \\\n",
        "    --article_month_path=gs://${BUCKET}/${DATASET}/article_month_list.txt \\\n",
        "    --save_tb_log_to_bucket \\\n",
        "    --bucket_tb_log_path=gs://${BUCKET}/tensorboard_log"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jobId: hybrid_recsys_train_job_200816_180754\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Job [hybrid_recsys_train_job_200816_180754] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs describe hybrid_recsys_train_job_200816_180754\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs stream-logs hybrid_recsys_train_job_200816_180754\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldzKFSuWZ7hh",
        "colab_type": "text"
      },
      "source": [
        "The training log should look like the following. \n",
        "\n",
        "<img src=\"img/hybrid_train_log.png\" width=\"80%\" height=\"80%\" />\n",
        "\n",
        "The final test result is loss: 4.471430, acc: 9.715167, top_10_acc: 42.097153. The top 10 accuracy is around 42.09%, which means our model has 42.09% chance to correctly predict the next news article the visitor would like to view if our hybrid recommendation model recommend 10 items. If randomly picking 10 items from total 2421 news articles, the top 10 accuracy would only be 0.4130%. Our model has 100 times better top 10 accuracy than random picking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpvVcrRGZ7hh",
        "colab_type": "text"
      },
      "source": [
        "In Coud shell, type ``` tensorboard --logdir=gs://hybrid-recsys-gcp-bucket/tensorboard_log --port=8080 ``` to launch tensorboard. Click the Web Preview button in Coud shell to open Tensorboard.\n",
        "\n",
        "<img src=\"img/tensorboard.png\" width=\"80%\" height=\"80%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PamQAseGZ7hh",
        "colab_type": "text"
      },
      "source": [
        "## 6. create serving model on gcloud\n",
        "\n",
        "Create AI Platform Model and set model version for serving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lhuJmZiZ7hi",
        "colab_type": "code",
        "colab": {},
        "outputId": "4a28b695-6d5e-4b58-f9cc-0bae501da3f5"
      },
      "source": [
        "%%bash\n",
        "\n",
        "MODEL_PATH=$(gsutil ls gs://$BUCKET/$MODEL/ | tail -1)\n",
        "\n",
        "echo $MODEL_PATH\n",
        "\n",
        "gcloud ai-platform models create ${MODEL} --regions=us-central1\n",
        "\n",
        "gcloud ai-platform versions create ${VERSION} \\\n",
        "    --model ${MODEL} \\\n",
        "    --staging-bucket gs://${BUCKET} \\\n",
        "    --origin ${MODEL_PATH} \\\n",
        "    --framework 'tensorflow' \\\n",
        "    --runtime-version 2.1 \\\n",
        "    --python-version 3.7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://hybrid-recsys-gcp-bucket/hybrid_recsys_trained_model/20200816182739/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using endpoint [https://ml.googleapis.com/]\n",
            "Created ml engine model [projects/hybrid-recsys-gcp/models/hybrid_recsys_trained_model].\n",
            "Using endpoint [https://ml.googleapis.com/]\n",
            "Creating version (this might take a few minutes)......\n",
            "...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o544WthqZ7hk",
        "colab_type": "text"
      },
      "source": [
        "## 7. batch prediction on gcloud\n",
        "\n",
        "Submit a predition job in gcloud to perform batch prediction for \"batch_pred_inputs.json\". Batch prediction is optimized for handling large json file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je7e967vZ7hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_PRED_FOLDER=\"hybrid_recsys_batch_pred\"\n",
        "os.environ[\"BATCH_PRED_FOLDER\"] = BATCH_PRED_FOLDER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcCFKK7UZ7hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_input_json(dataset_path, sample_num, inputs_json_name, label_name):\n",
        "    \"\"\" Samples from dataset and reate input json file and label txt file for prediction.\n",
        "\n",
        "    Args:\n",
        "        dataset_path (str): Path to csv dataset in gcp bucket.\n",
        "        sample_num (int): Number of samples to draw.\n",
        "        inputs_json_name (str): Name of json file to store input for prediction.\n",
        "        label_name (str): Name of txt file storing labels.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    script = \"gsutil cp {} ./dataset.csv\".format(dataset_path)\n",
        "    os.system(script)\n",
        "\n",
        "    df = pd.read_csv(\"./dataset.csv\")\n",
        "    df = df.sample(n=sample_num)\n",
        "    df = df.astype({'user_id':'str', 'item_id':'str', 'next_item_id':'str', 'article_year':'str', 'article_month':'str'})\n",
        "    df = df.drop(columns=['rating', 'fold'])\n",
        "    label = df.pop('next_item_id')\n",
        "    \n",
        "    inputs_text = df.to_json(orient='records')[1:-1].replace('},{', '} \\n {')\n",
        "    with open(inputs_json_name, 'w') as f:\n",
        "        f.write(inputs_text)\n",
        "        \n",
        "    label_text = '\\n'.join(list(label))\n",
        "    with open(label_name, 'w') as f:\n",
        "        f.write(label_text)\n",
        "    \n",
        "    script = \"rm ./dataset.csv\"\n",
        "    os.system(script)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTu1yCwjZ7hp",
        "colab_type": "code",
        "colab": {},
        "outputId": "afe60db4-01ad-43d7-9615-46f024f07317"
      },
      "source": [
        "dataset_path=\"gs://{}/{}/{}_test.csv\".format(BUCKET, DATASET, TABLE)\n",
        "inputs_json_name=\"batch_pred_inputs.json\"\n",
        "label_name=\"batch_pred_labels.txt\"\n",
        "sample_num = 2000\n",
        "\n",
        "# create prediction inputs\n",
        "create_input_json(dataset_path, sample_num, inputs_json_name, label_name)\n",
        "\n",
        "# copy files to folder and bucket\n",
        "!mkdir -p $BATCH_PRED_FOLDER\n",
        "!mv ./batch_pred_inputs.json ./$BATCH_PRED_FOLDER\n",
        "!mv ./batch_pred_labels.txt ./$BATCH_PRED_FOLDER\n",
        "!gsutil cp -r ./$BATCH_PRED_FOLDER/* gs://$BUCKET/$BATCH_PRED_FOLDER/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://./hybrid_recsys_batch_pred/batch_pred_inputs.json [Content-Type=application/json]...\n",
            "Copying file://./hybrid_recsys_batch_pred/batch_pred_labels.txt [Content-Type=text/plain]...\n",
            "/ [2 files][  2.6 MiB/  2.6 MiB]                                                \n",
            "Operation completed over 2 objects/2.6 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdlow4e0Z7hr",
        "colab_type": "code",
        "colab": {},
        "outputId": "abc06bde-e879-4d2e-c8ed-f0b78c36ae52"
      },
      "source": [
        "# view first input sample\n",
        "!head -1 ./hybrid_recsys_batch_pred/batch_pred_inputs.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"user_id\":\"3320141323412082760\",\"item_id\":\"299410466\",\"title\":\"Carfentanil: Der \\u201eserial killer\\u201c ist in \\u00d6sterreich aufgetaucht\",\"author\":\"Thomas  Trescher\",\"category\":\"News\",\"device_brand\":\"unknown\",\"article_year\":\"2017\",\"article_month\":\"11\",\"user_latent_0\":0.06162132,\"user_latent_1\":0.063703045,\"user_latent_2\":-0.09893329,\"user_latent_3\":-0.0016731527,\"user_latent_4\":0.07737582,\"user_latent_5\":0.017496314,\"user_latent_6\":0.12973891,\"user_latent_7\":-0.08892761,\"user_latent_8\":0.025988577,\"user_latent_9\":0.115164414,\"user_latent_10\":-0.0409198,\"user_latent_11\":-0.017002014,\"user_latent_12\":0.07399137,\"user_latent_13\":-0.0038234235,\"user_latent_14\":-0.015566276,\"user_latent_15\":-0.054921035,\"user_latent_16\":-0.0587562,\"user_latent_17\":0.017126346,\"user_latent_18\":0.023995874,\"user_latent_19\":0.04627099,\"item_latent_0\":-0.4167901,\"item_latent_1\":-0.9307941,\"item_latent_2\":0.87530863,\"item_latent_3\":0.93042785,\"item_latent_4\":0.86253023,\"item_latent_5\":0.42429322,\"item_latent_6\":-0.8550206,\"item_latent_7\":-0.20069851,\"item_latent_8\":0.08767344,\"item_latent_9\":-0.003735825,\"item_latent_10\":0.034636773,\"item_latent_11\":-0.06864103,\"item_latent_12\":-0.031046925,\"item_latent_13\":0.0283355,\"item_latent_14\":0.0022077707,\"item_latent_15\":0.038466208,\"item_latent_16\":-0.106701076,\"item_latent_17\":-0.07551355,\"item_latent_18\":0.040824503,\"item_latent_19\":-0.06269423} \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLG3ruv-Z7ht",
        "colab_type": "code",
        "colab": {},
        "outputId": "7a819e4b-90f7-4aee-a0ef-a562331945dd"
      },
      "source": [
        "%%bash\n",
        "\n",
        "INPUT=gs://${BUCKET}/${BATCH_PRED_FOLDER}/batch_pred_inputs.json\n",
        "OUTPUT=gs://${BUCKET}/${BATCH_PRED_FOLDER}/output/\n",
        "JOBID=hybrid_recsys_pred_job_$(date -u +%y%m%d_%H%M%S)\n",
        "\n",
        "gsutil rm -rf $OUTPUT\n",
        "\n",
        "gcloud ai-platform jobs submit prediction ${JOBID} \\\n",
        "  --data-format TEXT \\\n",
        "  --region ${REGION} \\\n",
        "  --input-paths ${INPUT} \\\n",
        "  --output-path ${OUTPUT} \\\n",
        "  --model ${MODEL} \\\n",
        "  --version ${VERSION} \\\n",
        "  --max-worker-count 8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jobId: hybrid_recsys_pred_job_200816_190500\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "CommandException: 1 files/objects could not be removed.\n",
            "Job [hybrid_recsys_pred_job_200816_190500] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs describe hybrid_recsys_pred_job_200816_190500\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs stream-logs hybrid_recsys_pred_job_200816_190500\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qPdmIwzZ7hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_batch_pred_acc(folder_name, result_path, label_path):\n",
        "    \"\"\" Copy prediction result and label to folder in current path. Calculate and print the accuracy \n",
        "    for batch prediction result\n",
        "\n",
        "    Args:\n",
        "        folder_name (str): Name of the folder to work.\n",
        "        result_path (str): Path to prediction result file.\n",
        "        label_path (str): Path to true label file.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    script2 = \"mkdir -p ./{}/output/\".format(folder_name)\n",
        "    script1 = \"rm -r ./{}/output/*\".format(folder_name)\n",
        "    script3 = \"gsutil cp {} ./{}/output/\".format(result_path, folder_name)\n",
        "    os.system(\"{} & {} & {}\".format(script1, script2, script3))\n",
        "    \n",
        "    filenames = sorted(glob.glob(\"./{}/output/prediction.results*\".format(folder_name)))\n",
        "    with open(\"./{}/output/batch_pred_result.txt\".format(folder_name), 'w') as outputfile:\n",
        "        for file in filenames:\n",
        "            with open(file, 'r') as inputfile:\n",
        "                for line in inputfile:\n",
        "                    outputfile.write(line)\n",
        "                    \n",
        "    script = \"gsutil cp {} ./{}/output/batch_pred_labels.txt\".format(label_path, folder_name)\n",
        "    os.system(script)\n",
        "    \n",
        "    match = 0\n",
        "    count = 0\n",
        "    with open(\"./{}/output/batch_pred_result.txt\".format(folder_name)) as f1, open(\"./{}/output/batch_pred_labels.txt\".format(folder_name)) as f2:\n",
        "        for line1, line2 in zip(f1, f2):\n",
        "            count += 1\n",
        "            if line2.strip() in line1:\n",
        "                match += 1\n",
        "    \n",
        "    print(\"Accuracy: {:.2f} %\".format(100*match/count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MRDKPzKZ7hx",
        "colab_type": "code",
        "colab": {},
        "outputId": "76ceaef6-35c1-4724-c97f-b48993879d77"
      },
      "source": [
        "batch_pred_result_path=\"gs://{}/{}/output/prediction.results*\".format(BUCKET, BATCH_PRED_FOLDER)\n",
        "batch_pred_label_path=\"gs://{}/{}/batch_pred_labels.txt\".format(BUCKET, BATCH_PRED_FOLDER)\n",
        "check_batch_pred_acc(BATCH_PRED_FOLDER, batch_pred_result_path, batch_pred_label_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 42.10 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlKUEmOZZ7hz",
        "colab_type": "text"
      },
      "source": [
        "The batch prediction result for 2000 samples is has top 10 accuracy around 42.10%, which is similar top 10 accuracy during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDc1BCY3Z7h0",
        "colab_type": "text"
      },
      "source": [
        "## 8. online prediction on gcloud\n",
        "\n",
        "Use gcloud to sends \"online_pred_inputs.json\" as json string, and the gcloud will parse and print the response message in terminal. The online prediction is optimized for minimizing latency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajWYRAOzZ7h0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ONLINE_PRED_FOLDER=\"hybrid_recsys_online_pred\"\n",
        "os.environ[\"ONLINE_PRED_FOLDER\"] = ONLINE_PRED_FOLDER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ittiFhxjZ7h5",
        "colab_type": "code",
        "colab": {},
        "outputId": "8a42f04e-34c9-47ab-a52e-18939a16294f"
      },
      "source": [
        "dataset_path=\"gs://{}/{}/{}_test.csv\".format(BUCKET, DATASET, TABLE)\n",
        "inputs_json_name=\"online_pred_inputs.json\"\n",
        "label_name=\"online_pred_labels.txt\"\n",
        "sample_num = 10\n",
        "\n",
        "# create prediction inputs\n",
        "create_input_json(dataset_path, sample_num, inputs_json_name, label_name)\n",
        "\n",
        "# copy files to folder and bucket\n",
        "!mkdir -p $ONLINE_PRED_FOLDER\n",
        "!mv ./online_pred_inputs.json ./$ONLINE_PRED_FOLDER\n",
        "!mv ./online_pred_labels.txt ./$ONLINE_PRED_FOLDER\n",
        "!gsutil cp -r ./$ONLINE_PRED_FOLDER/* gs://$BUCKET/$ONLINE_PRED_FOLDER/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://./hybrid_recsys_online_pred/online_pred_inputs.json [Content-Type=application/json]...\n",
            "Copying file://./hybrid_recsys_online_pred/online_pred_labels.txt [Content-Type=text/plain]...\n",
            "/ [2 files][ 13.5 KiB/ 13.5 KiB]                                                \n",
            "Operation completed over 2 objects/13.5 KiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnxGAp5dZ7h7",
        "colab_type": "code",
        "colab": {},
        "outputId": "0e217d3d-c7c5-4b53-e93d-8347424c1989"
      },
      "source": [
        "# view first input sample\n",
        "!head -1 ./hybrid_recsys_online_pred/online_pred_inputs.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"user_id\":\"1668201806318949252\",\"item_id\":\"299816215\",\"title\":\"Fahnenskandal von Mailand: Die Austria zeigt Flagge\",\"author\":\"Alexander Strecha\",\"category\":\"News\",\"device_brand\":\"unknown\",\"article_year\":\"2017\",\"article_month\":\"11\",\"user_latent_0\":0.029992696,\"user_latent_1\":0.030825611,\"user_latent_2\":-0.17452683,\"user_latent_3\":-0.014093319,\"user_latent_4\":0.007055789,\"user_latent_5\":-0.10078142,\"user_latent_6\":-0.04647213,\"user_latent_7\":0.0017855432,\"user_latent_8\":0.07790457,\"user_latent_9\":-0.031338274,\"user_latent_10\":-0.04410556,\"user_latent_11\":0.02756868,\"user_latent_12\":0.050546274,\"user_latent_13\":0.009949617,\"user_latent_14\":0.0140742855,\"user_latent_15\":0.0010659785,\"user_latent_16\":-0.05308226,\"user_latent_17\":-0.04033819,\"user_latent_18\":-0.013850107,\"user_latent_19\":0.020958075,\"item_latent_0\":-0.8666197,\"item_latent_1\":-0.036664557,\"item_latent_2\":0.8462881,\"item_latent_3\":0.818939,\"item_latent_4\":0.8356234,\"item_latent_5\":0.84413326,\"item_latent_6\":0.8936087,\"item_latent_7\":0.7776551,\"item_latent_8\":-0.74996305,\"item_latent_9\":0.754349,\"item_latent_10\":-0.004065779,\"item_latent_11\":-0.0320565,\"item_latent_12\":-0.009786883,\"item_latent_13\":0.035324477,\"item_latent_14\":-0.025341403,\"item_latent_15\":-0.04091143,\"item_latent_16\":-0.008959504,\"item_latent_17\":-0.0023605004,\"item_latent_18\":0.018848391,\"item_latent_19\":-0.0347714} \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4zeZmiVZ7h9",
        "colab_type": "code",
        "colab": {},
        "outputId": "230e0777-0586-4cd7-82f7-ae8b1489a880"
      },
      "source": [
        "%%bash\n",
        "\n",
        "JSON_PATH=./${ONLINE_PRED_FOLDER}/online_pred_inputs.json\n",
        "\n",
        "gcloud ai-platform predict \\\n",
        "  --model ${MODEL} \\\n",
        "  --version ${VERSION} \\\n",
        "  --json-instances ${JSON_PATH}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOP_K_ITEM_ID\n",
            "[u'299816215', u'187077794', u'299826775', u'299913368', u'299410466', u'299853016', u'299907204', u'299848776', u'299826767', u'299925700']\n",
            "[u'299865757', u'299910994', u'299826775', u'299410466', u'299913368', u'299836255', u'299816215', u'299917726', u'299925086', u'299931241']\n",
            "[u'299935287', u'299953030', u'299943426', u'299957318', u'299931241', u'299949793', u'299935266', u'299930679', u'299826775', u'299949869']\n",
            "[u'299949793', u'299935287', u'299941050', u'299930679', u'299925700', u'299826775', u'299939900', u'299934703', u'299922662', u'299936493']\n",
            "[u'299914459', u'299809748', u'299937546', u'299837992', u'299907275', u'299903877', u'299942840', u'299804319', u'299907267', u'299814183']\n",
            "[u'299816215', u'299866366', u'299836255', u'299824032', u'299826767', u'299836841', u'299837992', u'299800704', u'299910994', u'299833840']\n",
            "[u'299865757', u'299959410', u'299836255', u'299910994', u'299410466', u'299931241', u'299933565', u'299826775', u'298972803', u'299917726']\n",
            "[u'299911496', u'299933565', u'299953030', u'299866366', u'299836841', u'299410466', u'299902870', u'299899819', u'299906166', u'299965853']\n",
            "[u'299957318', u'299410466', u'299931241', u'299933565', u'299953030', u'299826775', u'299925700', u'299930679', u'299816215', u'299917726']\n",
            "[u'181585672', u'299814775', u'299826775', u'299931241', u'299907275', u'299914459', u'299410466', u'247092153', u'299837992', u'299925700']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using endpoint [https://ml.googleapis.com/]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo8oOebkZ7h_",
        "colab_type": "text"
      },
      "source": [
        "## 9. prediction with REST API\n",
        "\n",
        "Send HTTP request to model API and receive HTTP response to get prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVgs-SYbZ7h_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_input_data(dataset_path, sample_num):\n",
        "    script = \"gsutil cp {} ./dataset.csv\".format(dataset_path)\n",
        "    os.system(script)\n",
        "    \n",
        "    df = pd.read_csv(\"./dataset.csv\")\n",
        "    df = df.sample(n=sample_num)\n",
        "    df = df.astype({'user_id':'str', 'item_id':'str', 'next_item_id':'str', 'article_year':'str', 'article_month':'str'})\n",
        "    df = df.drop(columns=['rating', 'fold'])\n",
        "    label = df.pop('next_item_id')\n",
        "    \n",
        "    script = \"rm ./dataset.csv\"\n",
        "    os.system(script)\n",
        "    \n",
        "    return {'instances':df.to_dict('records')}, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VJuuUfgZ7iB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path=\"gs://{}/{}/{}_test.csv\".format(BUCKET, DATASET, TABLE)\n",
        "sample_num = 5\n",
        "\n",
        "data3, label = create_input_data(dataset_path, sample_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg_0GrkOZ7iD",
        "colab_type": "code",
        "colab": {},
        "outputId": "47541653-871b-4671-be4b-f509101fc554"
      },
      "source": [
        "# view first sample\n",
        "data3['instances'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_id': '1729997426586066411',\n",
              " 'item_id': '299804319',\n",
              " 'title': 'Matthias Reim spricht über Absturz & Comeback',\n",
              " 'author': 'Elisabeth Spitzer',\n",
              " 'category': 'Stars & Kultur',\n",
              " 'device_brand': 'unknown',\n",
              " 'article_year': '2017',\n",
              " 'article_month': '11',\n",
              " 'user_latent_0': -0.011381128,\n",
              " 'user_latent_1': 0.0917952,\n",
              " 'user_latent_2': -0.16476962,\n",
              " 'user_latent_3': 0.0122773675,\n",
              " 'user_latent_4': 0.08191427,\n",
              " 'user_latent_5': -0.031006693999999998,\n",
              " 'user_latent_6': -0.014491842,\n",
              " 'user_latent_7': 0.10038319,\n",
              " 'user_latent_8': 0.11502589,\n",
              " 'user_latent_9': 0.03494215,\n",
              " 'user_latent_10': 0.033939923999999996,\n",
              " 'user_latent_11': 0.020114326999999998,\n",
              " 'user_latent_12': -0.015681986000000002,\n",
              " 'user_latent_13': -0.010152572,\n",
              " 'user_latent_14': 0.0059823147,\n",
              " 'user_latent_15': 0.065449744,\n",
              " 'user_latent_16': 0.039926145,\n",
              " 'user_latent_17': -0.0044548786,\n",
              " 'user_latent_18': -0.015749516,\n",
              " 'user_latent_19': -0.013907265,\n",
              " 'item_latent_0': 0.44690087,\n",
              " 'item_latent_1': -0.35489124,\n",
              " 'item_latent_2': -0.7970297,\n",
              " 'item_latent_3': 0.6606028,\n",
              " 'item_latent_4': -0.6210468000000001,\n",
              " 'item_latent_5': 0.7846092,\n",
              " 'item_latent_6': -0.7568933000000001,\n",
              " 'item_latent_7': 0.5468225,\n",
              " 'item_latent_8': 0.78378665,\n",
              " 'item_latent_9': 0.8358538000000001,\n",
              " 'item_latent_10': -0.024003497999999998,\n",
              " 'item_latent_11': -0.07005069400000001,\n",
              " 'item_latent_12': -0.046481203,\n",
              " 'item_latent_13': 0.0030826865999999996,\n",
              " 'item_latent_14': 0.043491222,\n",
              " 'item_latent_15': -0.073984295,\n",
              " 'item_latent_16': 0.011897308,\n",
              " 'item_latent_17': -0.0032521382,\n",
              " 'item_latent_18': 0.053923856,\n",
              " 'item_latent_19': 0.07460823}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aeCKv5dZ7iF",
        "colab_type": "code",
        "colab": {},
        "outputId": "a8d51fb1-2b9c-4506-c1e4-d1d3e1563079"
      },
      "source": [
        "# prepare aip\n",
        "token = GoogleCredentials.get_application_default().get_access_token().access_token\n",
        "api = 'https://ml.googleapis.com/v1/projects/{}/models/{}/versions/{}:predict'.format(PROJECT, MODEL, VERSION)\n",
        "headers = {'Authorization': 'Bearer ' + token }\n",
        "\n",
        "# make request\n",
        "response = requests.post(api, json=data3, headers=headers)\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'{\"predictions\": [{\"top_k_item_id\": [\"299914459\", \"299809748\", \"299903877\", \"299837992\", \"299791583\", \"299907267\", \"299804319\", \"299779564\", \"299800661\", \"299826767\"]}, {\"top_k_item_id\": [\"299816215\", \"299853016\", \"299802551\", \"299410466\", \"299824032\", \"299922662\", \"299808560\", \"299836255\", \"299804319\", \"299918253\"]}, {\"top_k_item_id\": [\"299902870\", \"299866366\", \"299836255\", \"299865757\", \"299899819\", \"299826775\", \"299898026\", \"299816215\", \"299933565\", \"299836841\"]}, {\"top_k_item_id\": [\"299826775\", \"299931241\", \"299925700\", \"299957318\", \"299933565\", \"299410466\", \"299836255\", \"299816215\", \"299913368\", \"299943437\"]}, {\"top_k_item_id\": [\"299814842\", \"299852437\", \"299836255\", \"299866366\", \"299410466\", \"299865757\", \"299853016\", \"299836841\", \"299933565\", \"299844825\"]}]}'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEKFaCi0Z7iH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByddHkPnZ7iJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}