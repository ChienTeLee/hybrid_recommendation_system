{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4k18nivWAPG"
   },
   "source": [
    "# Extract User and Item Latent Factors Using Neural Collaborative Filtering\n",
    "\n",
    "Here we train neural collaborative filtering (NCF) model to predict the ratings given the user_ids and item_ids. The neural collaborative filtering model consists of Generalized Matrix Factorization (GMF) stream and Multi-Layer Perceptron (MLP) stream, which represents the matrix factorization and the non-linear relation of the user embedding and the item embedding. The neural collaborative filtering model fuses GMF stream and MLP stream and is able to predict the ratings given the ids of the user and the item. After training, the user and item embeddings from GMF and MLP stream are treated as user and item latent factors. The latent factors are stored into \"user_latent.csv\" and \"item_latent.csv\" in GCP bucket.\n",
    "\n",
    "---\n",
    "The implementation is related to the following paper:\n",
    "- [Neural Collaborative Filtering](https://arxiv.org/abs/1708.05031)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants\n",
    "PROJECT = \"hybrid-recsys-gcp\"\n",
    "BUCKET = \"hybrid-recsys-gcp-bucket\"\n",
    "REGION = 'us-central1'\n",
    "DATASET = 'news_recommend_dataset'\n",
    "MODEL = \"neural_collaborate_filter_trained_model\"\n",
    "\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"DATASET\"] = DATASET\n",
    "os.environ[\"MODEL\"] = MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. create neural_collaborate_filter package\n",
    "\n",
    "This is the package for the neural colaborative filtering model. The trainer/task.py defines the input arguments for training. In trainer/model.py, the \"create_dataset\" function creates tf.dataset for input user and item ids. The \"NeuMF_Model\" class defines the tf.keras.Model which takes user and item id to predict rating. And the \"train_model_and_save_latent_factors\" function defines the custom training loop for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NeuMF_Model uses the following architecture:\n",
    "\n",
    "<img src=\"./img/neural_CF.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_weFIvdVwp5o"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p neural_collaborate_filter/trainer\n",
    "touch neural_collaborate_filter/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing neural_collaborate_filter/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile neural_collaborate_filter/trainer/task.py\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from trainer import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"job dir to store training outputs and other data\",\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"path to import train data\",\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--test_data_path\",\n",
    "        help=\"path to import test data\",\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"output dir to export checkpoints or trained model\",\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"batch size for training\",\n",
    "        type=int,\n",
    "        default=2048\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        help=\"number of epochs for training\",\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--latent_num\",\n",
    "        help=\"number of latent factors for gmf and mlp each\",\n",
    "        type=int,\n",
    "        default=8\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--user_id_path\",\n",
    "        help=\"path to import user_id_list.txt\",\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--item_id_path\",\n",
    "        help=\"path to import item_id_list.txt\",\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--user_latent_path\",\n",
    "        help=\"output path to save user latent factors\",\n",
    "        default=\"./\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--item_latent_path\",\n",
    "        help=\"output path to save item latent factors\",\n",
    "        default=\"./\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--save_latent_factors\",\n",
    "        help=\"set to save latent factors\",\n",
    "        default=False,\n",
    "        action=\"store_true\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    args = args.__dict__\n",
    "    \n",
    "    model.train_model_and_save_latent_factors(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing neural_collaborate_filter/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile neural_collaborate_filter/trainer/model.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "# create dataset function\n",
    "def create_dataset(path, column_names, label_name, defaults, batch_size, shuffle):\n",
    "    \"\"\" Create tf.dataset from csv file.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the csv file.\n",
    "        column_names (list:str): List of string to specify which columns to use in dataset (including label).\n",
    "        label_name (str): Column name for the label.\n",
    "        defaults (list:str): List of string to set default values for columns.\n",
    "        batch_size (str): Batchsize of the dataset.\n",
    "        shuffle (bool): True for shuffling dataset and False otherwise.\n",
    "\n",
    "    Returns:\n",
    "        (tf.dataset): dataset used for training or testing\n",
    "    \"\"\"\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=path,\n",
    "        select_columns=column_names,\n",
    "        label_name=label_name,\n",
    "        column_defaults = defaults,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=1,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "# model class\n",
    "class NeuMF_Model(tf.keras.Model):\n",
    "    \"\"\" The NeuMF_Model class. Takes user ids and item ids to predict ratings through matrix factorization and \n",
    "    multilayer perceptron. The user and item embeddings are user and item latent factors.\n",
    "\n",
    "    Attributes:\n",
    "        user_index_table (tf.lookup.StaticVocabularyTable): Table to transform user_id to user indices.\n",
    "        item_index_table (tf.lookup.StaticVocabularyTable): Table to transform item_id to item indices.\n",
    "        \n",
    "        gmf_u_embed (tf.keras.layers.Embedding): User embedding layer for General Matrix Factorization.\n",
    "        gmf_i_embed (tf.keras.layers.Embedding): Item embedding layer for General Matrix Factorization.\n",
    "        mlp_u_embed (tf.keras.layers.Embedding): User embedding layer for Multilayer Perceptron.\n",
    "        mlp_i_embed (tf.keras.layers.Embedding): Item embedding layer for Multilayer Perceptron.\n",
    "        \n",
    "        dense_1 (tf.keras.layers.Dense): First dense layer of multilayer perceptron stream.\n",
    "        dense_2 (tf.keras.layers.Dense): Second dense layer of multilayer perceptron stream.\n",
    "        dense_3 (tf.keras.layers.Dense): Third dense layer of multilayer perceptron stream.\n",
    "        dense_4 (tf.keras.layers.Dense): Fourth dense layer of multilayer perceptron stream.\n",
    "        output_layer (tf.keras.layers.Dense): Output dense layer for concat of mlp and gmf stream.\n",
    "        \n",
    "        mlp_concat (tf.keras.layers.Concatenate): Concatenate layer to combine user and item embedding in mlp stream.\n",
    "        stream_concat (tf.keras.layers.Concatenate): Concatenate layer to combine mlp and gmf stream. \n",
    "    \"\"\"\n",
    "    def __init__(self, user_file_path, item_file_path, latent_num):\n",
    "        \"\"\" init method for NeuMF_Model class\n",
    "        \n",
    "        Args:\n",
    "            user_file_path (str): Path to txt file containing user ids.\n",
    "            item_file_path (str): Path to txt file containing item ids.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(NeuMF_Model, self).__init__()\n",
    "        self.user_index_table = self.create_lookup_table(user_file_path)\n",
    "        self.item_index_table = self.create_lookup_table(item_file_path)\n",
    "        \n",
    "        user_id_size = self.get_size(user_file_path)\n",
    "        item_id_size = self.get_size(item_file_path)\n",
    "        self.gmf_u_embed = tf.keras.layers.Embedding(user_id_size, latent_num, name='gmf_u_embed')\n",
    "        self.gmf_i_embed = tf.keras.layers.Embedding(item_id_size, latent_num, name='gmf_i_embed')\n",
    "        self.mlp_u_embed = tf.keras.layers.Embedding(user_id_size, latent_num, name='mlp_u_embed')\n",
    "        self.mlp_i_embed = tf.keras.layers.Embedding(item_id_size, latent_num, name='mlp_i_embed')\n",
    "\n",
    "        self.dense_1 = tf.keras.layers.Dense(64, activation='relu', name='mlp_dense_1')\n",
    "        self.dense_2 = tf.keras.layers.Dense(32, activation='relu', name='mlp_dense_2')\n",
    "        self.dense_3 = tf.keras.layers.Dense(16, activation='relu', name='mlp_dense_3')\n",
    "        self.dense_4 = tf.keras.layers.Dense(8, activation='relu', name='mlp_dense_4')\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='output_layer')\n",
    "        \n",
    "        self.mlp_concat = tf.keras.layers.Concatenate(axis=1, name='mlp_concat')\n",
    "        self.stream_concat = tf.keras.layers.Concatenate(axis=1, name='stream_concat')\n",
    "\n",
    "    def create_lookup_table(self, file_path):\n",
    "        \"\"\" create lookup table to translate ids to indices\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to txt file containing ids.\n",
    "            \n",
    "        Returns:\n",
    "            (tf.lookup.StaticVocabularyTable): The lookup table.\n",
    "        \"\"\"\n",
    "        file_initializer = tf.lookup.TextFileInitializer(file_path, key_dtype=tf.string, key_index=tf.lookup.TextFileIndex.WHOLE_LINE, \\\n",
    "                            value_dtype=tf.int64, value_index=tf.lookup.TextFileIndex.LINE_NUMBER, delimiter=\"\\n\")\n",
    "        lookup_table = tf.lookup.StaticVocabularyTable(file_initializer, num_oov_buckets=1)\n",
    "        return lookup_table\n",
    "    \n",
    "    def get_size(self, file_path):\n",
    "        \"\"\" Get the total number of lines for a txt file, indicating the size of the column.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to txt file.\n",
    "            \n",
    "        Returns:\n",
    "            (int): The total number of lines for the txt file.\n",
    "        \"\"\"\n",
    "        id_text = tf.io.read_file(file_path)\n",
    "        id_tensor = tf.strings.split(id_text, '\\n')\n",
    "        return id_tensor.shape[0]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training):\n",
    "        \"\"\"The call method for NeuMF_Model class.\n",
    "\n",
    "        Args:\n",
    "            inputs (OrderedDict:tf.Tensor): OrderedDict of tensor containing user_id and item_id\n",
    "        \n",
    "        Returns:\n",
    "            output (tf.Tensor): The predicted rating for the user and item combination.\n",
    "        \"\"\"\n",
    "        user_id = inputs['user_id']\n",
    "        item_id = inputs['item_id']\n",
    "\n",
    "        # convert id to index\n",
    "        user_index = self.user_index_table.lookup(user_id)\n",
    "        item_index = self.item_index_table.lookup(item_id)\n",
    "\n",
    "        # GMF stream\n",
    "        gmf_u_latent = self.gmf_u_embed(user_index)\n",
    "        gmf_i_latent = self.gmf_i_embed(item_index)\n",
    "\n",
    "        # multiply latent factors\n",
    "        gmf_out = gmf_u_latent * gmf_i_latent\n",
    "\n",
    "        # MLP stream\n",
    "        mlp_u_latent = self.mlp_u_embed(user_index)\n",
    "        mlp_i_latent = self.mlp_i_embed(item_index)\n",
    "\n",
    "        # concat latent factors and pass to dense layers\n",
    "        mlp_concat_out = self.mlp_concat([mlp_u_latent, mlp_i_latent])\n",
    "        dense_1_out = self.dense_1(mlp_concat_out)\n",
    "        dense_2_out = self.dense_2(dense_1_out)\n",
    "        dense_3_out = self.dense_3(dense_2_out)\n",
    "        mlp_out = self.dense_4(dense_3_out)\n",
    "\n",
    "        # concat GMF and MLP stream\n",
    "        stream_concat_out = self.stream_concat([gmf_out, mlp_out])\n",
    "\n",
    "        output = self.output_layer(stream_concat_out)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def save_latent_factors_to_bucket(col_name, col_path, tensor_weight, output_path, latent_num):\n",
    "    \"\"\"Store tensor weights as user or item latent factors to bucket in csv file.\n",
    "\n",
    "    Args:\n",
    "        col_name (str): Column name for the latent factors (user or item).\n",
    "        col_path (str): Path to user or item ids.\n",
    "        tensor_weight (tf.Tensor): Tensors of the embedding layer used as latent factors.\n",
    "        output_path (str): Path to ouput file in bucket.\n",
    "        latent_num (int): Number of latent factors\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    id_tensors = tf.strings.split(tf.io.read_file(col_path), \"\\n\")\n",
    "    id_list = [tf.compat.as_str_any(x) for x in id_tensors.numpy()]\n",
    "\n",
    "    latent_df = pd.DataFrame(tensor_weight)\n",
    "    latent_df[col_name] = id_list\n",
    "\n",
    "    key = range(latent_num * 2)\n",
    "    value = ['{}_latent_'.format(col_name[0]) + str(x) for x in key]\n",
    "    column_dict = dict(zip(key, value))\n",
    "    \n",
    "    latent_df = latent_df.rename(columns=column_dict)\n",
    "    latent_df = latent_df[[col_name] + value]\n",
    "    latent_df.to_csv(\"./latent.csv\", index=False)\n",
    "    \n",
    "    script = \"gsutil mv ./latent.csv {}\".format(output_path)\n",
    "    os.system(script)\n",
    "    \n",
    "    \n",
    "def train_model_and_save_latent_factors(args):\n",
    "    \"\"\" Train the NeuMF_Model and save embeddings as latent_factors to bucket in csv files.\n",
    "\n",
    "    Args:\n",
    "        args (dict): dict of arguments from task.py\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # create dataset\n",
    "    column_name = ['user_id', 'item_id', 'rating']\n",
    "    label_name = 'rating'\n",
    "    defaults = ['unknown', 'unknown', 0.0]\n",
    "    batch_size = args[\"batch_size\"]\n",
    "    train_path = args[\"train_data_path\"]\n",
    "    test_path = args[\"test_data_path\"]\n",
    "    \n",
    "    train_dataset = create_dataset(train_path, column_name, label_name, defaults, batch_size, True)\n",
    "    test_dataset = create_dataset(test_path, column_name, label_name, defaults, batch_size, False)\n",
    "    \n",
    "    # create model\n",
    "    model = NeuMF_Model(args[\"user_id_path\"], args[\"item_id_path\"], args[\"latent_num\"])\n",
    "    \n",
    "    # loss function and optimizers\n",
    "    bc_loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    \n",
    "    # loss metrics\n",
    "    train_bc_loss = tf.keras.metrics.Mean(name='train_bc_loss')\n",
    "    train_mae_loss = tf.keras.metrics.MeanAbsoluteError(name='train_mae_loss')\n",
    "    train_rmse_loss = tf.keras.metrics.RootMeanSquaredError(name='train_rmse_loss')\n",
    "\n",
    "    test_bc_loss = tf.keras.metrics.Mean(name='test_bc_loss')\n",
    "    test_mae_loss = tf.keras.metrics.MeanAbsoluteError(name='test_mae_loss')\n",
    "    test_rmse_loss = tf.keras.metrics.RootMeanSquaredError(name='test_rmse_loss')\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(features, labels):\n",
    "        \"\"\" Concrete function for train setp and update train metircs\n",
    "\n",
    "        Args:\n",
    "            features (OrderedDict:tf.Tensor): OrderedDict of tensor containing user_id and item_id as features.\n",
    "            labels (tf.Tensor): labels (rating) of the training examples\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(features, training=True)\n",
    "            bc_loss = bc_loss_object(labels, preds)\n",
    "        gradients = tape.gradient(bc_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        train_bc_loss(bc_loss)\n",
    "        train_mae_loss(labels, preds)\n",
    "        train_rmse_loss(labels, preds)\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(features, labels):\n",
    "        \"\"\" Concrete function for test setp and update test metircs\n",
    "\n",
    "        Args:\n",
    "            features (OrderedDict:tf.Tensor): OrderedDict of tensor containing user_id and item_id as features.\n",
    "            labels (tf.Tensor): labels (rating) of the training examples\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        preds = model(features, training=False)\n",
    "        bc_loss = bc_loss_object(labels, preds)\n",
    "        test_bc_loss(bc_loss)\n",
    "        test_mae_loss(labels, preds)\n",
    "        test_rmse_loss(labels, preds)\n",
    "    \n",
    "    # custom train loop\n",
    "    EPOCHS = args[\"epochs\"]\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_bc_loss.reset_states()\n",
    "        train_mae_loss.reset_states()\n",
    "        train_rmse_loss.reset_states()\n",
    "\n",
    "        test_bc_loss.reset_states()\n",
    "        test_mae_loss.reset_states()\n",
    "        test_rmse_loss.reset_states()\n",
    "\n",
    "        for features, labels in train_dataset:\n",
    "            train_step(features, labels)\n",
    "\n",
    "        for features, labels in test_dataset:\n",
    "            test_step(features, labels)\n",
    "\n",
    "        template = \"Epoch {:d}, train [bc_loss: {:.5f}, mae_loss: {:.5f}, rmse_loss: {:.5f}], test [bc_loss: {:.5f}, mae_loss: {:.5f}, rmse_loss: {:.5f}]\"\n",
    "        print(template.format(epoch + 1, train_bc_loss.result(), train_mae_loss.result(), train_rmse_loss.result(), \\\n",
    "                                test_bc_loss.result(), test_mae_loss.result(), test_rmse_loss.result()))\n",
    "        \n",
    "    # export model\n",
    "    EXPORT_PATH = os.path.join(args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(obj=model, export_dir=EXPORT_PATH)\n",
    "    \n",
    "    if args[\"save_latent_factors\"]:\n",
    "        # get embedding weights\n",
    "        user_weight = tf.concat([model.gmf_u_embed.get_weights()[0], model.mlp_u_embed.get_weights()[0]], axis = 1).numpy()\n",
    "        item_weight = tf.concat([model.gmf_i_embed.get_weights()[0], model.mlp_i_embed.get_weights()[0]], axis = 1).numpy()\n",
    "\n",
    "        # store embedding weights to csv\n",
    "        save_latent_factors_to_bucket('user_id', args[\"user_id_path\"], user_weight, args[\"user_latent_path\"], args[\"latent_num\"])\n",
    "        save_latent_factors_to_bucket('item_id', args[\"item_id_path\"], item_weight, args[\"item_latent_path\"], args[\"latent_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. train model locally\n",
    "\n",
    "Run package as a python module in local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train [bc_loss: 0.68107, mae_loss: 0.32247, rmse_loss: 0.35924], test [bc_loss: 0.65923, mae_loss: 0.30864, rmse_loss: 0.34384]\n",
      "Epoch 2, train [bc_loss: 0.63461, mae_loss: 0.28303, rmse_loss: 0.32761], test [bc_loss: 0.63296, mae_loss: 0.26911, rmse_loss: 0.32561]\n",
      "Epoch 3, train [bc_loss: 0.59234, mae_loss: 0.24501, rmse_loss: 0.29762], test [bc_loss: 0.62794, mae_loss: 0.25887, rmse_loss: 0.32119]\n",
      "Epoch 4, train [bc_loss: 0.57712, mae_loss: 0.23196, rmse_loss: 0.28702], test [bc_loss: 0.62337, mae_loss: 0.25424, rmse_loss: 0.31821]\n",
      "Epoch 5, train [bc_loss: 0.56766, mae_loss: 0.22499, rmse_loss: 0.28077], test [bc_loss: 0.61896, mae_loss: 0.25087, rmse_loss: 0.31504]\n",
      "Epoch 6, train [bc_loss: 0.55217, mae_loss: 0.21330, rmse_loss: 0.26984], test [bc_loss: 0.61631, mae_loss: 0.24596, rmse_loss: 0.31236]\n",
      "Epoch 7, train [bc_loss: 0.53069, mae_loss: 0.19702, rmse_loss: 0.25319], test [bc_loss: 0.61959, mae_loss: 0.24323, rmse_loss: 0.31293]\n",
      "Epoch 8, train [bc_loss: 0.50990, mae_loss: 0.18110, rmse_loss: 0.23642], test [bc_loss: 0.61970, mae_loss: 0.24115, rmse_loss: 0.31259]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-16 17:19:47.948743: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2020-08-16 17:19:47.949785: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c95a30de90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-08-16 17:19:47.949822: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-08-16 17:19:47.950096: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function NeuMF_Model.call at 0x7f49f4c8a710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "2020-08-16 17:20:37.281640: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "JOBDIR=./${MODEL}\n",
    "OUTDIR=./${MODEL}\n",
    "\n",
    "rm -rf ${JOBDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/neural_collaborate_filter\n",
    "\n",
    "python -m trainer.task \\\n",
    "    --job-dir=${JOBDIR} \\\n",
    "    --train_data_path=gs://${BUCKET}/${DATASET}/preprocess_train.csv \\\n",
    "    --test_data_path=gs://${BUCKET}/${DATASET}/preprocess_test.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=2048 \\\n",
    "    --epochs=8 \\\n",
    "    --latent_num=10 \\\n",
    "    --user_id_path=gs://${BUCKET}/${DATASET}/user_id_list.txt \\\n",
    "    --item_id_path=gs://${BUCKET}/${DATASET}/item_id_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. train model on gcloud\n",
    "\n",
    "Submit a training job in gcloud ai-platform to train the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: neural_collaborate_filter_train_job_200816_172050\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [neural_collaborate_filter_train_job_200816_172050] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe neural_collaborate_filter_train_job_200816_172050\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs neural_collaborate_filter_train_job_200816_172050\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "JOBDIR=gs://${BUCKET}/${MODEL}\n",
    "OUTDIR=gs://${BUCKET}/${MODEL}\n",
    "JOBID=neural_collaborate_filter_train_job_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBID} \\\n",
    "    --region=${REGION} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/neural_collaborate_filter/trainer \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    --master-machine-type=n1-highcpu-16 \\\n",
    "    --runtime-version=2.1 \\\n",
    "    --python-version=3.7 \\\n",
    "    -- \\\n",
    "    --job-dir=${JOBDIR} \\\n",
    "    --train_data_path=gs://${BUCKET}/${DATASET}/preprocess_train.csv \\\n",
    "    --test_data_path=gs://${BUCKET}/${DATASET}/preprocess_test.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=2048 \\\n",
    "    --epochs=8 \\\n",
    "    --latent_num=10 \\\n",
    "    --user_id_path=gs://${BUCKET}/${DATASET}/user_id_list.txt \\\n",
    "    --item_id_path=gs://${BUCKET}/${DATASET}/item_id_list.txt \\\n",
    "    --user_latent_path=gs://${BUCKET}/${DATASET}/user_latent.csv \\\n",
    "    --item_latent_path=gs://${BUCKET}/${DATASET}/item_latent.csv \\\n",
    "    --save_latent_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ai-platform training log should look like the following. The final test result is: bc_loss: 0.60900, mae_loss: 0.24119, rmse_loss: 0.30784.\n",
    "\n",
    "<img src=\"./img/ncf_train_log.png\" width=\"80%\" height=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. save latent factors in bigquery dataset\n",
    "\n",
    "Save user and item laten factors into \"user_latent.csv\" and \"item_latent.csv\" in GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_bigquery_table(project_id, dataset_id, table_id, schema, soruce_uri):\n",
    "    \"\"\" Load content from csv file into bigquery table.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): ID of the project.\n",
    "        dataset_id (str): ID of the dataset.\n",
    "        table_id (str): ID of the table.\n",
    "        schema (list:bigquery.SchemaField): Schema of the csv file.\n",
    "        soruce_uri (str): Path to the csv file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project_id)\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.schema = schema\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_EMPTY\n",
    "    job_config.skip_leading_rows = 1\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    \n",
    "    load_job = client.load_table_from_uri(soruce_uri, dataset_ref.table(table_id), job_config=job_config)\n",
    "    print(\"Starting job {}\".format(load_job.job_id))\n",
    "\n",
    "    load_job.result()  # Waits for table load to complete.\n",
    "    print(\"Job finished.\")\n",
    "\n",
    "    destination_table = client.get_table(dataset_ref.table(table_id))\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 93be1920-f263-4a6c-9b3e-6fc7a750e100\n",
      "Job finished.\n",
      "Loaded 16313 rows.\n",
      "Starting job ef578c38-4c96-4f9c-a70f-985cce1c30fe\n",
      "Job finished.\n",
      "Loaded 2421 rows.\n"
     ]
    }
   ],
   "source": [
    "latent_num = 10\n",
    "\n",
    "user_schema = [bigquery.SchemaField(\"user_id\", \"STRING\", mode=\"REQUIRED\")] + \\\n",
    "            [bigquery.SchemaField(\"user_latent_{}\".format(i), \"FLOAT\", mode=\"REQUIRED\") for i in range(2 * latent_num)]\n",
    "\n",
    "item_schema = [bigquery.SchemaField(\"item_id\", \"STRING\", mode=\"REQUIRED\")] + \\\n",
    "            [bigquery.SchemaField(\"item_latent_{}\".format(i), \"FLOAT\", mode=\"REQUIRED\") for i in range(2 * latent_num)]\n",
    "\n",
    "load_csv_to_bigquery_table(PROJECT, DATASET, \"user_latent\", user_schema, \"gs://{}/{}/user_latent.csv\".format(BUCKET, DATASET))\n",
    "load_csv_to_bigquery_table(PROJECT, DATASET, \"item_latent\", item_schema, \"gs://{}/{}/item_latent.csv\".format(BUCKET, DATASET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. view user and item latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://hybrid-recsys-gcp-bucket/news_recommend_dataset/user_latent.csv...\n",
      "/ [1 files][  4.1 MiB/  4.1 MiB]                                                \n",
      "Operation completed over 1 objects/4.1 MiB.                                      \n",
      "Copying gs://hybrid-recsys-gcp-bucket/news_recommend_dataset/item_latent.csv...\n",
      "/ [1 files][594.3 KiB/594.3 KiB]                                                \n",
      "Operation completed over 1 objects/594.3 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://{BUCKET}/{DATASET}/user_latent.csv ./{DATASET}/user_latent.csv\n",
    "!gsutil cp gs://{BUCKET}/{DATASET}/item_latent.csv ./{DATASET}/item_latent.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(\"./{}/user_latent.csv\".format(DATASET))\n",
    "item_df = pd.read_csv(\"./{}/item_latent.csv\".format(DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>u_latent_0</th>\n",
       "      <th>u_latent_1</th>\n",
       "      <th>u_latent_2</th>\n",
       "      <th>u_latent_3</th>\n",
       "      <th>u_latent_4</th>\n",
       "      <th>u_latent_5</th>\n",
       "      <th>u_latent_6</th>\n",
       "      <th>u_latent_7</th>\n",
       "      <th>u_latent_8</th>\n",
       "      <th>...</th>\n",
       "      <th>u_latent_10</th>\n",
       "      <th>u_latent_11</th>\n",
       "      <th>u_latent_12</th>\n",
       "      <th>u_latent_13</th>\n",
       "      <th>u_latent_14</th>\n",
       "      <th>u_latent_15</th>\n",
       "      <th>u_latent_16</th>\n",
       "      <th>u_latent_17</th>\n",
       "      <th>u_latent_18</th>\n",
       "      <th>u_latent_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000163602560555666</td>\n",
       "      <td>-0.223890</td>\n",
       "      <td>0.013522</td>\n",
       "      <td>-0.197976</td>\n",
       "      <td>0.217497</td>\n",
       "      <td>-0.053681</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>-0.117004</td>\n",
       "      <td>0.113265</td>\n",
       "      <td>0.187496</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043796</td>\n",
       "      <td>-0.084263</td>\n",
       "      <td>-0.035453</td>\n",
       "      <td>0.039801</td>\n",
       "      <td>-0.007067</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>-0.042641</td>\n",
       "      <td>-0.033181</td>\n",
       "      <td>0.028918</td>\n",
       "      <td>0.054818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000196974485173657</td>\n",
       "      <td>-0.033306</td>\n",
       "      <td>0.020547</td>\n",
       "      <td>0.104502</td>\n",
       "      <td>-0.003414</td>\n",
       "      <td>0.063732</td>\n",
       "      <td>0.086023</td>\n",
       "      <td>-0.062370</td>\n",
       "      <td>0.030699</td>\n",
       "      <td>-0.115149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034464</td>\n",
       "      <td>-0.007665</td>\n",
       "      <td>-0.092310</td>\n",
       "      <td>-0.005157</td>\n",
       "      <td>0.033543</td>\n",
       "      <td>-0.067541</td>\n",
       "      <td>-0.027155</td>\n",
       "      <td>0.054411</td>\n",
       "      <td>0.026142</td>\n",
       "      <td>0.006107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002090131595000997</td>\n",
       "      <td>-0.179897</td>\n",
       "      <td>-0.139295</td>\n",
       "      <td>0.073862</td>\n",
       "      <td>-0.047588</td>\n",
       "      <td>0.047952</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>0.117391</td>\n",
       "      <td>0.058213</td>\n",
       "      <td>-0.077938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012818</td>\n",
       "      <td>-0.001946</td>\n",
       "      <td>0.033016</td>\n",
       "      <td>0.063178</td>\n",
       "      <td>0.066878</td>\n",
       "      <td>-0.069696</td>\n",
       "      <td>-0.025472</td>\n",
       "      <td>-0.085891</td>\n",
       "      <td>0.034603</td>\n",
       "      <td>0.034044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002109532017576768</td>\n",
       "      <td>-0.079408</td>\n",
       "      <td>-0.174885</td>\n",
       "      <td>0.014121</td>\n",
       "      <td>-0.081578</td>\n",
       "      <td>0.140167</td>\n",
       "      <td>-0.137453</td>\n",
       "      <td>0.088288</td>\n",
       "      <td>0.162533</td>\n",
       "      <td>-0.106551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016511</td>\n",
       "      <td>0.027259</td>\n",
       "      <td>-0.065256</td>\n",
       "      <td>-0.023040</td>\n",
       "      <td>-0.099605</td>\n",
       "      <td>0.058657</td>\n",
       "      <td>0.018901</td>\n",
       "      <td>0.037335</td>\n",
       "      <td>-0.014985</td>\n",
       "      <td>-0.017714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004209053768679755</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>-0.134218</td>\n",
       "      <td>0.076557</td>\n",
       "      <td>-0.169822</td>\n",
       "      <td>-0.072396</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>-0.026878</td>\n",
       "      <td>-0.070867</td>\n",
       "      <td>0.092746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.044778</td>\n",
       "      <td>0.013842</td>\n",
       "      <td>0.023910</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.046790</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>-0.023228</td>\n",
       "      <td>0.038247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id  u_latent_0  u_latent_1  u_latent_2  u_latent_3  \\\n",
       "0  1000163602560555666   -0.223890    0.013522   -0.197976    0.217497   \n",
       "1  1000196974485173657   -0.033306    0.020547    0.104502   -0.003414   \n",
       "2  1002090131595000997   -0.179897   -0.139295    0.073862   -0.047588   \n",
       "3  1002109532017576768   -0.079408   -0.174885    0.014121   -0.081578   \n",
       "4  1004209053768679755   -0.000192   -0.134218    0.076557   -0.169822   \n",
       "\n",
       "   u_latent_4  u_latent_5  u_latent_6  u_latent_7  u_latent_8  ...  \\\n",
       "0   -0.053681   -0.006720   -0.117004    0.113265    0.187496  ...   \n",
       "1    0.063732    0.086023   -0.062370    0.030699   -0.115149  ...   \n",
       "2    0.047952   -0.000489    0.117391    0.058213   -0.077938  ...   \n",
       "3    0.140167   -0.137453    0.088288    0.162533   -0.106551  ...   \n",
       "4   -0.072396    0.000815   -0.026878   -0.070867    0.092746  ...   \n",
       "\n",
       "   u_latent_10  u_latent_11  u_latent_12  u_latent_13  u_latent_14  \\\n",
       "0    -0.043796    -0.084263    -0.035453     0.039801    -0.007067   \n",
       "1    -0.034464    -0.007665    -0.092310    -0.005157     0.033543   \n",
       "2    -0.012818    -0.001946     0.033016     0.063178     0.066878   \n",
       "3     0.016511     0.027259    -0.065256    -0.023040    -0.099605   \n",
       "4     0.002242     0.044778     0.013842     0.023910     0.012564   \n",
       "\n",
       "   u_latent_15  u_latent_16  u_latent_17  u_latent_18  u_latent_19  \n",
       "0     0.016380    -0.042641    -0.033181     0.028918     0.054818  \n",
       "1    -0.067541    -0.027155     0.054411     0.026142     0.006107  \n",
       "2    -0.069696    -0.025472    -0.085891     0.034603     0.034044  \n",
       "3     0.058657     0.018901     0.037335    -0.014985    -0.017714  \n",
       "4     0.051200     0.046790     0.003449    -0.023228     0.038247  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>i_latent_0</th>\n",
       "      <th>i_latent_1</th>\n",
       "      <th>i_latent_2</th>\n",
       "      <th>i_latent_3</th>\n",
       "      <th>i_latent_4</th>\n",
       "      <th>i_latent_5</th>\n",
       "      <th>i_latent_6</th>\n",
       "      <th>i_latent_7</th>\n",
       "      <th>i_latent_8</th>\n",
       "      <th>...</th>\n",
       "      <th>i_latent_10</th>\n",
       "      <th>i_latent_11</th>\n",
       "      <th>i_latent_12</th>\n",
       "      <th>i_latent_13</th>\n",
       "      <th>i_latent_14</th>\n",
       "      <th>i_latent_15</th>\n",
       "      <th>i_latent_16</th>\n",
       "      <th>i_latent_17</th>\n",
       "      <th>i_latent_18</th>\n",
       "      <th>i_latent_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100170790</td>\n",
       "      <td>-0.044401</td>\n",
       "      <td>-0.054478</td>\n",
       "      <td>-0.024215</td>\n",
       "      <td>-0.095297</td>\n",
       "      <td>0.030977</td>\n",
       "      <td>-0.051534</td>\n",
       "      <td>-0.087727</td>\n",
       "      <td>0.066595</td>\n",
       "      <td>-0.116718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>-0.022293</td>\n",
       "      <td>-0.038569</td>\n",
       "      <td>0.042152</td>\n",
       "      <td>-0.046344</td>\n",
       "      <td>0.025076</td>\n",
       "      <td>-0.079884</td>\n",
       "      <td>-0.023431</td>\n",
       "      <td>0.031445</td>\n",
       "      <td>-0.052044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100292889</td>\n",
       "      <td>0.044174</td>\n",
       "      <td>0.018957</td>\n",
       "      <td>-0.020329</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>-0.066686</td>\n",
       "      <td>-0.046977</td>\n",
       "      <td>-0.011907</td>\n",
       "      <td>0.023122</td>\n",
       "      <td>-0.024344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048059</td>\n",
       "      <td>-0.057492</td>\n",
       "      <td>-0.052943</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>-0.061653</td>\n",
       "      <td>0.076567</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>-0.028304</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>-0.075180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100735153</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>-0.092585</td>\n",
       "      <td>-0.101787</td>\n",
       "      <td>-0.067878</td>\n",
       "      <td>0.077632</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>-0.068222</td>\n",
       "      <td>0.012467</td>\n",
       "      <td>-0.053971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001989</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>0.056933</td>\n",
       "      <td>-0.082168</td>\n",
       "      <td>0.075539</td>\n",
       "      <td>-0.025724</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.067758</td>\n",
       "      <td>-0.043770</td>\n",
       "      <td>-0.009335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100915139</td>\n",
       "      <td>0.009406</td>\n",
       "      <td>0.015461</td>\n",
       "      <td>-0.031027</td>\n",
       "      <td>-0.006515</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>-0.004458</td>\n",
       "      <td>0.006125</td>\n",
       "      <td>-0.020394</td>\n",
       "      <td>-0.046054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044172</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>-0.025532</td>\n",
       "      <td>-0.008830</td>\n",
       "      <td>-0.017709</td>\n",
       "      <td>0.037827</td>\n",
       "      <td>0.035382</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>-0.034189</td>\n",
       "      <td>-0.043504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101092112</td>\n",
       "      <td>-0.063698</td>\n",
       "      <td>0.059048</td>\n",
       "      <td>0.049322</td>\n",
       "      <td>-0.023419</td>\n",
       "      <td>0.039215</td>\n",
       "      <td>0.036990</td>\n",
       "      <td>0.013302</td>\n",
       "      <td>-0.031852</td>\n",
       "      <td>-0.001982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043176</td>\n",
       "      <td>-0.017732</td>\n",
       "      <td>-0.049217</td>\n",
       "      <td>0.024714</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.061918</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>-0.008226</td>\n",
       "      <td>0.010462</td>\n",
       "      <td>0.009586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_id  i_latent_0  i_latent_1  i_latent_2  i_latent_3  i_latent_4  \\\n",
       "0  100170790   -0.044401   -0.054478   -0.024215   -0.095297    0.030977   \n",
       "1  100292889    0.044174    0.018957   -0.020329    0.005043   -0.066686   \n",
       "2  100735153    0.004435   -0.092585   -0.101787   -0.067878    0.077632   \n",
       "3  100915139    0.009406    0.015461   -0.031027   -0.006515    0.015776   \n",
       "4  101092112   -0.063698    0.059048    0.049322   -0.023419    0.039215   \n",
       "\n",
       "   i_latent_5  i_latent_6  i_latent_7  i_latent_8  ...  i_latent_10  \\\n",
       "0   -0.051534   -0.087727    0.066595   -0.116718  ...     0.027045   \n",
       "1   -0.046977   -0.011907    0.023122   -0.024344  ...     0.048059   \n",
       "2    0.000198   -0.068222    0.012467   -0.053971  ...    -0.001989   \n",
       "3   -0.004458    0.006125   -0.020394   -0.046054  ...     0.044172   \n",
       "4    0.036990    0.013302   -0.031852   -0.001982  ...     0.043176   \n",
       "\n",
       "   i_latent_11  i_latent_12  i_latent_13  i_latent_14  i_latent_15  \\\n",
       "0    -0.022293    -0.038569     0.042152    -0.046344     0.025076   \n",
       "1    -0.057492    -0.052943     0.007809    -0.061653     0.076567   \n",
       "2     0.011519     0.056933    -0.082168     0.075539    -0.025724   \n",
       "3     0.006846    -0.025532    -0.008830    -0.017709     0.037827   \n",
       "4    -0.017732    -0.049217     0.024714     0.000673     0.061918   \n",
       "\n",
       "   i_latent_16  i_latent_17  i_latent_18  i_latent_19  \n",
       "0    -0.079884    -0.023431     0.031445    -0.052044  \n",
       "1     0.009918    -0.028304     0.027838    -0.075180  \n",
       "2     0.038086     0.067758    -0.043770    -0.009335  \n",
       "3     0.035382     0.005540    -0.034189    -0.043504  \n",
       "4     0.006136    -0.008226     0.010462     0.009586  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "collaborative_filtering.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
