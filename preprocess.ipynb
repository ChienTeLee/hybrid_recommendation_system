{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the for the news_recommend_dataset\n",
    "\n",
    "In this notebook, we are going to use standard SQL to extract features from BigQuey dataset, and build our own preprocess dataset . The \"[cloud-training-demos.GA360_test.ga_sessions_sample](https://console.cloud.google.com/bigquery?p=cloud-training-demos&d=GA360_test&t=ga_sessions_sample&page=table)\" is a public BigQuey dataset containing the Google Analytics data from Austrian news website [Kurier.at](https://kurier.at/). We query the \"cloud-training-demos.GA360_test.ga_sessions_sample\" in Bigquery, and take out customDimensions such as item_id, user_id, news title, news author, news date, mobile device brand, or other content-based features. The features are then written into preprocess dataset named \"preprocess_train.csv\" and \"preprocess_text.csv\" which will be stored in GCS bucket.\n",
    "\n",
    "The preprocess dataset will later be used for training neural collaborative filtering model to extract user and item embeddings as latent factors. After that, combine preprocess dataset and laten factors into news_recommend_dataset. The news_recommend_dataset will later be used for training hybrid recommendation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants\n",
    "PROJECT = \"hybrid-recsys-gcp\"\n",
    "BUCKET = \"hybrid-recsys-gcp-bucket\"\n",
    "DATASET = \"news_recommend_dataset\"\n",
    "TABLE = \"preprocess\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. create BigQuery dataset and table\n",
    "\n",
    "Select customDimension such as user_id, item_id, title, author, and category as features. Use the time spent on each page as the implicit rating for the news. Apply farm_figerprint to hash user_id and visit_time into hash_id, and use the modulo of hash_id for splitting train and test sets. Finally, save the extracted train and test tables in Bigquery, and save the csv files in GCS bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted features with discriptions:\n",
    "\n",
    "| Feature | Discription |\n",
    "| :- | :- |\n",
    "| user_id | ID of the visitor. |\n",
    "| item_id | ID of the news article. |\n",
    "| title | Title of the news article. |\n",
    "| author | Author of the news article. |\n",
    "| device_brand | Brand of the device used by visitor to view the news. |\n",
    "| article_year | Year of the news article. |\n",
    "| article_month | Month of the news article. |\n",
    "| rating | Implicit rating for the news. Calculated using time spent on each news article. |\n",
    "| next_item_id | Next news article viewed by the visitor. |\n",
    "| fold | Modulo of farm_fingerprint hash of user_id and visit_time . Used for splitting train and test sets |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset in bigquery\n",
    "def create_bigquery_dataset(project_id, dataset_id):\n",
    "    \"\"\" Create dataset in bigquery under the  project.\n",
    "    \n",
    "    Args:\n",
    "        project_id (str): The ID of your project.\n",
    "        dataset_id (str): The name for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    dataset = bigquery.Dataset(\"{}.{}\".format(project_id, dataset_id))\n",
    "    dataset = client.create_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bigquery_dataset(PROJECT, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify query for preprocessing train dataset\n",
    "preprocess_train_query = \"\"\"\n",
    "WITH user_count_table AS (\n",
    "    SELECT \n",
    "        fullVisitorId as user_id,\n",
    "        Count(*) - 1 AS intaractions\n",
    "\n",
    "    FROM cloud-training-demos.GA360_test.ga_sessions_sample,\n",
    "        UNNEST(hits) AS hits\n",
    "\n",
    "    WHERE\n",
    "        fullVisitorId IS NOT NULL\n",
    "        AND\n",
    "        (SELECT value FROM  hits.customDimensions WHERE index = 10) IS NOT NULL\n",
    "        AND\n",
    "        visitStartTime IS NOT NULL\n",
    "        AND\n",
    "        hits.time IS NOT NULL\n",
    "        AND\n",
    "        hits.type = \"PAGE\"\n",
    "        \n",
    "    GROUP BY fullVisitorId\n",
    "),\n",
    "\n",
    "raw_data_table AS(\n",
    "    SELECT\n",
    "        fullVisitorId as user_id,\n",
    "        (SELECT value FROM  hits.customDimensions WHERE index = 10) AS item_id,\n",
    "        (SELECT REPLACE(value, \",\", \" \") FROM  hits.customDimensions WHERE index = 6) AS title,\n",
    "        (SELECT REGEXP_EXTRACT(value, r\"^[^,]+\") FROM  hits.customDimensions WHERE index = 2) AS author,\n",
    "        (SELECT value FROM  hits.customDimensions WHERE index = 7) AS category,\n",
    "        (SELECT value FROM  hits.customDimensions WHERE index = 4) AS article_date,\n",
    "        device.mobileDeviceBranding AS device_brand,\n",
    "        visitStartTime * 1000 + hits.time AS hits_timestamp,\n",
    "        (LEAD(visitStartTime * 1000 + hits.time, 1) OVER (PARTITION BY fullVisitorId ORDER BY (visitStartTime * 1000 + hits.time) ASC) - (visitStartTime * 1000 + hits.time)) AS time_span,\n",
    "        LEAD((SELECT value FROM  hits.customDimensions WHERE index = 10), 1) OVER (PARTITION BY fullVisitorId ORDER BY (visitStartTime * 1000 + hits.time) ASC) AS next_item_id,\n",
    "        intaractions,\n",
    "        FARM_FINGERPRINT(CONCAT(fullVisitorId, visitStartTime * 1000 + hits.time)) AS hash_id,\n",
    "        CONCAT(fullVisitorId, (SELECT value FROM  hits.customDimensions WHERE index = 10)) AS user_item_id\n",
    "        \n",
    "    FROM cloud-training-demos.GA360_test.ga_sessions_sample,   \n",
    "        UNNEST(hits) AS hits\n",
    "        \n",
    "    INNER JOIN user_count_table\n",
    "    ON fullVisitorId = user_count_table.user_id\n",
    "        \n",
    "    WHERE\n",
    "        fullVisitorId IS NOT NULL\n",
    "        AND\n",
    "        (SELECT value FROM  hits.customDimensions WHERE index = 10) IS NOT NULL\n",
    "        AND\n",
    "        visitStartTime IS NOT NULL\n",
    "        AND\n",
    "        hits.time IS NOT NULL\n",
    "        AND\n",
    "        hits.type = \"PAGE\"\n",
    "        AND\n",
    "        intaractions > 5\n",
    "),\n",
    "\n",
    "max_time_table AS (\n",
    "    SELECT\n",
    "        user_item_id,\n",
    "        max(time_span) AS max_time,\n",
    "        PERCENTILE_CONT(max(time_span), 0.5) OVER() AS median\n",
    "        \n",
    "    FROM raw_data_table\n",
    "    \n",
    "    GROUP BY user_item_id\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    user_id,\n",
    "    item_id,\n",
    "    IFNULL(title, 'unknown') AS title,\n",
    "    IFNULL(author, 'unknown') AS author,\n",
    "    IFNULL(category, 'unknown') AS category,\n",
    "    IFNULL(device_brand, 'unknown') AS device_brand,\n",
    "    SUBSTR(article_date, 1, 4) AS article_year,\n",
    "    SUBSTR(article_date, 6, 2) AS article_month,\n",
    "    IF (0.5 * max_time / median > 1.0, 1.0, 0.5 * max_time / median) AS rating,\n",
    "    next_item_id,\n",
    "    ABS(MOD(hash_id, 10)) AS fold\n",
    "    \n",
    "FROM \n",
    "    raw_data_table\n",
    "\n",
    "INNER JOIN max_time_table\n",
    "ON raw_data_table.user_item_id= max_time_table.user_item_id\n",
    "\n",
    "WHERE\n",
    "    next_item_id IS NOT NULL\n",
    "    AND\n",
    "    time_span IS NOT NULL\n",
    "    AND\n",
    "    ABS(MOD(hash_id, 10)) > 0\n",
    "\n",
    "ORDER BY user_id ASC, hits_timestamp ASC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify query for preprocessing test dataset\n",
    "preprocess_test_query = \"\"\"\n",
    "WITH user_count_table AS (\n",
    "    SELECT \n",
    "        fullVisitorId as user_id,\n",
    "        Count(*) - 1 AS intaractions\n",
    "\n",
    "    FROM cloud-training-demos.GA360_test.ga_sessions_sample,\n",
    "        UNNEST(hits) AS hits\n",
    "\n",
    "    WHERE\n",
    "        fullVisitorId IS NOT NULL\n",
    "        AND\n",
    "        (SELECT value FROM  hits.customDimensions WHERE index = 10) IS NOT NULL\n",
    "        AND\n",
    "        visitStartTime IS NOT NULL\n",
    "        AND\n",
    "        hits.time IS NOT NULL\n",
    "        AND\n",
    "        hits.type = \"PAGE\"\n",
    "        \n",
    "    GROUP BY fullVisitorId\n",
    "),\n",
    "\n",
    "raw_data_table AS(\n",
    "    SELECT\n",
    "        fullVisitorId as user_id,\n",
    "        (SELECT value FROM  hits.customDimensions WHERE index = 10) AS item_id,\n",
    "        (SELECT REPLACE(value, \",\", \" \") FROM  hits.customDimensions WHERE index = 6) AS title,\n",
    "        (SELECT REGEXP_EXTRACT(value, r\"^[^,]+\") FROM  hits.customDimensions WHERE index = 2) AS author,\n",
    "        (SELECT value FROM  hits.customDimensions WHERE index = 7) AS category,\n",
    "        (SELECT value FROM  hits.customDimensions WHERE index = 4) AS article_date,\n",
    "        device.mobileDeviceBranding AS device_brand,\n",
    "        visitStartTime * 1000 + hits.time AS hits_timestamp,\n",
    "        (LEAD(visitStartTime * 1000 + hits.time, 1) OVER (PARTITION BY fullVisitorId ORDER BY (visitStartTime * 1000 + hits.time) ASC) - (visitStartTime * 1000 + hits.time)) AS time_span,\n",
    "        LEAD((SELECT value FROM  hits.customDimensions WHERE index = 10), 1) OVER (PARTITION BY fullVisitorId ORDER BY (visitStartTime * 1000 + hits.time) ASC) AS next_item_id,\n",
    "        intaractions,\n",
    "        FARM_FINGERPRINT(CONCAT(fullVisitorId, visitStartTime * 1000 + hits.time)) AS hash_id,\n",
    "        CONCAT(fullVisitorId, (SELECT value FROM  hits.customDimensions WHERE index = 10)) AS user_item_id\n",
    "        \n",
    "    FROM cloud-training-demos.GA360_test.ga_sessions_sample,   \n",
    "        UNNEST(hits) AS hits\n",
    "        \n",
    "    INNER JOIN user_count_table\n",
    "    ON fullVisitorId = user_count_table.user_id\n",
    "        \n",
    "    WHERE\n",
    "        fullVisitorId IS NOT NULL\n",
    "        AND\n",
    "        (SELECT value FROM  hits.customDimensions WHERE index = 10) IS NOT NULL\n",
    "        AND\n",
    "        visitStartTime IS NOT NULL\n",
    "        AND\n",
    "        hits.time IS NOT NULL\n",
    "        AND\n",
    "        hits.type = \"PAGE\"\n",
    "        AND\n",
    "        intaractions > 5\n",
    "),\n",
    "\n",
    "max_time_table AS (\n",
    "    SELECT\n",
    "        user_item_id,\n",
    "        max(time_span) AS max_time,\n",
    "        PERCENTILE_CONT(max(time_span), 0.5) OVER() AS median\n",
    "        \n",
    "    FROM raw_data_table\n",
    "    \n",
    "    GROUP BY user_item_id\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    user_id,\n",
    "    item_id,\n",
    "    IFNULL(title, 'unknown') AS title,\n",
    "    IFNULL(author, 'unknown') AS author,\n",
    "    IFNULL(category, 'unknown') AS category,\n",
    "    IFNULL(device_brand, 'unknown') AS device_brand,\n",
    "    SUBSTR(article_date, 1, 4) AS article_year,\n",
    "    SUBSTR(article_date, 6, 2) AS article_month,\n",
    "    IF (0.5 * max_time / median > 1.0, 1.0, 0.5 * max_time / median) AS rating,\n",
    "    next_item_id,\n",
    "    ABS(MOD(hash_id, 10)) AS fold\n",
    "    \n",
    "FROM \n",
    "    raw_data_table\n",
    "\n",
    "INNER JOIN max_time_table\n",
    "ON raw_data_table.user_item_id= max_time_table.user_item_id\n",
    "\n",
    "WHERE\n",
    "    next_item_id IS NOT NULL\n",
    "    AND\n",
    "    time_span IS NOT NULL\n",
    "    AND\n",
    "    ABS(MOD(hash_id, 10)) = 0\n",
    "\n",
    "ORDER BY user_id ASC, hits_timestamp ASC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify schema\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"user_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"item_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"title\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"author\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"category\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"device_brand\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"article_year\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"article_month\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"rating\", \"FLOAT\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"next_item_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"fold\", \"INTEGER\", mode=\"REQUIRED\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_query_result_to_table(query, project_id, dataset_id, table_id, bucket_id, schema, mode):\n",
    "    \"\"\" Execute query, store result in bigquery table, and save result to bucket in csv file.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query to be executed in bigquery.\n",
    "        project_id (str): The ID of your project.\n",
    "        dataset_id (str): The name of the dataset.\n",
    "        table_id (str): The name for the table.\n",
    "        bucket_id (str): Bucket to store the csv file of query result.\n",
    "        schema (list:bigquery.SchemaField): Schema of the query result.\n",
    "        mode (str): Train or test mode.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    table = bigquery.Table(\"{}.{}.{}\".format(project_id, dataset_id, table_id + \"_\" + mode), schema=schema)\n",
    "    table = client.create_table(table)\n",
    "    table_ref = client.dataset(dataset_id).table(table_id + \"_\" + mode)\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.destination = table_ref\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_EMPTY\n",
    "        \n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    result = query_job.result()\n",
    "    \n",
    "    destination_uri = \"gs://{}/{}/{}_{}.csv\".format(bucket_id, dataset_id, table_id, mode)\n",
    "    extract_job = client.extract_table(table_ref, destination_uri)\n",
    "    extract_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preprocess train and test tables in bigquery\n",
    "store_query_result_to_table(preprocess_train_query, PROJECT, DATASET, TABLE, BUCKET, schema, \"train\")\n",
    "store_query_result_to_table(preprocess_test_query, PROJECT, DATASET, TABLE, BUCKET, schema, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. view news_recommend_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://hybrid-recsys-gcp-bucket/news_recommend_dataset/preprocess_train.csv...\n",
      "/ [1 files][ 22.2 MiB/ 22.2 MiB]                                                \n",
      "Operation completed over 1 objects/22.2 MiB.                                     \n",
      "Copying gs://hybrid-recsys-gcp-bucket/news_recommend_dataset/preprocess_test.csv...\n",
      "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
      "Operation completed over 1 objects/2.5 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://{BUCKET}/{DATASET}/{TABLE}_train.csv ./{DATASET}/{TABLE}_train.csv\n",
    "!gsutil cp gs://{BUCKET}/{DATASET}/{TABLE}_test.csv ./{DATASET}/{TABLE}_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./{}/{}_train.csv\".format(DATASET, TABLE))\n",
    "test_df = pd.read_csv(\"./{}/{}_test.csv\".format(DATASET, TABLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>device_brand</th>\n",
       "      <th>article_year</th>\n",
       "      <th>article_month</th>\n",
       "      <th>rating</th>\n",
       "      <th>next_item_id</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000164e+18</td>\n",
       "      <td>299853016</td>\n",
       "      <td>Schröcksnadel gegen Werdenigg: Keine Aussprache</td>\n",
       "      <td>unknown</td>\n",
       "      <td>News</td>\n",
       "      <td>Apple</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>0.147207</td>\n",
       "      <td>298888038</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000164e+18</td>\n",
       "      <td>298888038</td>\n",
       "      <td>Investment kann jetzt jeder!</td>\n",
       "      <td>unknown</td>\n",
       "      <td>News</td>\n",
       "      <td>Apple</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>0.484673</td>\n",
       "      <td>299814775</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000164e+18</td>\n",
       "      <td>299814775</td>\n",
       "      <td>Meghan Markle: Verlobungsring aus Dianas Brosche</td>\n",
       "      <td>Maria Zelenko</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>Apple</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>0.500774</td>\n",
       "      <td>299772450</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000164e+18</td>\n",
       "      <td>299772450</td>\n",
       "      <td>Kritik an Prinz Harry: \"Ein verzogener Rotzlöf...</td>\n",
       "      <td>Elisabeth Spitzer</td>\n",
       "      <td>Stars &amp; Kultur</td>\n",
       "      <td>Apple</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>299824032</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000164e+18</td>\n",
       "      <td>299824032</td>\n",
       "      <td>YouTube: Schwere Probleme mit verstörenden Kin...</td>\n",
       "      <td>Georg Leyrer</td>\n",
       "      <td>Stars &amp; Kultur</td>\n",
       "      <td>Apple</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>0.857168</td>\n",
       "      <td>299809748</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    item_id                                              title  \\\n",
       "0  1.000164e+18  299853016    Schröcksnadel gegen Werdenigg: Keine Aussprache   \n",
       "1  1.000164e+18  298888038                       Investment kann jetzt jeder!   \n",
       "2  1.000164e+18  299814775   Meghan Markle: Verlobungsring aus Dianas Brosche   \n",
       "3  1.000164e+18  299772450  Kritik an Prinz Harry: \"Ein verzogener Rotzlöf...   \n",
       "4  1.000164e+18  299824032  YouTube: Schwere Probleme mit verstörenden Kin...   \n",
       "\n",
       "              author        category device_brand  article_year  \\\n",
       "0            unknown            News        Apple          2017   \n",
       "1            unknown            News        Apple          2017   \n",
       "2      Maria Zelenko       Lifestyle        Apple          2017   \n",
       "3  Elisabeth Spitzer  Stars & Kultur        Apple          2017   \n",
       "4       Georg Leyrer  Stars & Kultur        Apple          2017   \n",
       "\n",
       "   article_month    rating  next_item_id  fold  \n",
       "0             11  0.147207     298888038     6  \n",
       "1             11  0.484673     299814775     5  \n",
       "2             11  0.500774     299772450     6  \n",
       "3             11  1.000000     299824032     5  \n",
       "4             11  0.857168     299809748     3  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>device_brand</th>\n",
       "      <th>article_year</th>\n",
       "      <th>article_month</th>\n",
       "      <th>rating</th>\n",
       "      <th>next_item_id</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000196974485173657</td>\n",
       "      <td>299910994</td>\n",
       "      <td>Direktorensprecherin Isabella Zins: So könnte ...</td>\n",
       "      <td>Ute Brühl</td>\n",
       "      <td>News</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>299899819</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000196974485173657</td>\n",
       "      <td>299930679</td>\n",
       "      <td>Wintereinbruch naht: Erster Schnee im Osten mö...</td>\n",
       "      <td>Daniela Wahl</td>\n",
       "      <td>News</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>299972194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1004209053768679755</td>\n",
       "      <td>18976804</td>\n",
       "      <td>Heimskandal - Brigitte Wanker: Die Landesverrä...</td>\n",
       "      <td>Georg Hönigsberger</td>\n",
       "      <td>News</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>299695400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004555043399129313</td>\n",
       "      <td>299837992</td>\n",
       "      <td>Das erste TV-Interview von Prinz Harry &amp; Megha...</td>\n",
       "      <td>Christina Michlits</td>\n",
       "      <td>Stars &amp; Kultur</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>0.979912</td>\n",
       "      <td>299824032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004555043399129313</td>\n",
       "      <td>299836841</td>\n",
       "      <td>ÖVP will Studiengebühren  FPÖ in Verhandlungen...</td>\n",
       "      <td>Raffaela Lindorfer</td>\n",
       "      <td>News</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>299899819</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id    item_id  \\\n",
       "0  1000196974485173657  299910994   \n",
       "1  1000196974485173657  299930679   \n",
       "2  1004209053768679755   18976804   \n",
       "3  1004555043399129313  299837992   \n",
       "4  1004555043399129313  299836841   \n",
       "\n",
       "                                               title              author  \\\n",
       "0  Direktorensprecherin Isabella Zins: So könnte ...           Ute Brühl   \n",
       "1  Wintereinbruch naht: Erster Schnee im Osten mö...        Daniela Wahl   \n",
       "2  Heimskandal - Brigitte Wanker: Die Landesverrä...  Georg Hönigsberger   \n",
       "3  Das erste TV-Interview von Prinz Harry & Megha...  Christina Michlits   \n",
       "4  ÖVP will Studiengebühren  FPÖ in Verhandlungen...  Raffaela Lindorfer   \n",
       "\n",
       "         category device_brand  article_year  article_month    rating  \\\n",
       "0            News      unknown          2017             11  1.000000   \n",
       "1            News      unknown          2017             11  1.000000   \n",
       "2            News       Huawei          2013              7  1.000000   \n",
       "3  Stars & Kultur      unknown          2017             11  0.979912   \n",
       "4            News      unknown          2017             11  1.000000   \n",
       "\n",
       "   next_item_id  fold  \n",
       "0     299899819     0  \n",
       "1     299972194     0  \n",
       "2     299695400     0  \n",
       "3     299824032     0  \n",
       "4     299899819     0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. save unique column values to text\n",
    "\n",
    "Save unique values of each colum in text files in GCS bucket. They will later be used for creating feature columns in hybrid recommendaiton system model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_col_values(dataset_id, table_id, col_name, path):\n",
    "    \"\"\" Store unique values of the column of bigquery table into txt file. \n",
    "    \n",
    "    Args:\n",
    "        dataset_id (str): The name of the dataset.\n",
    "        table_id (str): The name of the table.\n",
    "        col_name (str): Name of the column.\n",
    "        path (str): Path to store the txt result.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    query = '''\n",
    "        SELECT DISTINCT \n",
    "            {} \n",
    "        FROM \n",
    "            {}.{}_train\n",
    "\n",
    "        UNION DISTINCT\n",
    "\n",
    "        SELECT \n",
    "            {}\n",
    "        FROM \n",
    "            {}.{}_test\n",
    "\n",
    "        ORDER BY {} ASC\n",
    "        '''.format(col_name, dataset_id, table_id, col_name, dataset_id, table_id, col_name)\n",
    "    \n",
    "    script = '''\n",
    "        bq query --format=csv --use_legacy_sql=false -n 100000 \"{}\" > query_result.txt &&\n",
    "        tail -n +2 query_result.txt > truncate_result.txt &&\n",
    "        truncate -s -1 truncate_result.txt\n",
    "        gsutil mv truncate_result.txt {}\n",
    "        rm query_result.txt\n",
    "        '''.format(query, path)\n",
    "    \n",
    "    os.system(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['user_id', 'item_id', 'author', 'category', 'device_brand', 'article_year', 'article_month']\n",
    "file_paths = [\"gs://{}/{}/{}_list.txt\".format(BUCKET, DATASET, x) for x in column_names]\n",
    "\n",
    "# for each column, save unique values to text\n",
    "for col, path in zip(column_names, file_paths):\n",
    "    get_unique_col_values(DATASET, TABLE, col, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
